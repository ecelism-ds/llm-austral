{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from numpy.linalg import norm\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = ''\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "En esta práctica implementaremos un sistema RAG (Retrieval-Augmented Generation).\n",
    "\n",
    "La revolución en torno a los LLM se basan en que escalando suficientemente un modelo basado en transformers como GPT-3 se ha descubierto que, si bien los modelos de completion no habían sido entrenado para razonar, ni responder preguntas, podían ser utilizados para ello!\n",
    "\n",
    "Estas capacidades se denominan capacidades emergentes, ya que emergen del entrenamiento y escalamiento de la arquitectura: no hay un cambio de arquitectura, ni de entrenamiento.\n",
    "\n",
    "Estas capacidades son propias del modelo, o sea están codificado en sus pesos. Dentro de los pesos se ha demostrado que se codifica conocimiento. Es así que los modelos pueden responder preguntas de conocimiento general. No obstante, si se quieren responder preguntas sobre data que no haya visto el modelo durante el entrenamiento (ya sea porque la información no haya estado disponible, sea privada o muy reciente o porque haya cambiado) entonces no responderá correctamente.\n",
    "\n",
    "#### Y si usamos fine-tunning ?\n",
    "\n",
    "Si bien es posible re-entrenar el modelo no es algo que sirva para agregar nuevo conocimiento: el conocimiento que adquieren estos modelos suele ser conocimiento que ve en reiteradas ocasiones en su corpus de entrenamiento. Además, como los pesos codifican otras capacidades que van desde básicas como razonar y manejar el lenguaje de forma coherente hasta conocimiento específico, modificar los pesos puede actuar en perjuicio de estas capacidades, desaprendiendo lo aprendido (problema conocido como \"castastrofic forgetting\"). Por otro lado, entrenarlos en corpus enteros como los que han sido usados para su creación lleva clusters de millones de dólares. El fine-tunning se suele hacer con técnicas que evitan el problema del \"castastrofic forgetting\" (como LoRa) y no para agregar nuevo conocimiento sino para afectar la manera en la que actúan (con técnicas como DPO o PPO). Para profundizar más pueden ver el siguiente tutorial https://huggingface.co/blog/dpo-trl\n",
    "\n",
    "## Volviendo sobre RAG\n",
    "\n",
    "Antes de los modelos generativos, la manera de responder consultas del usuario (estilo \"que día nación San Martin?\") sobre un corpus de datos, se solía encarar como un problema de Information Retrival: la respuesta consistía en hallar los textos y los párrafos en dichos textos que incluyera la información que buscaba el usuario.\n",
    "\n",
    "### Construyendo un dataset sintético\n",
    "\n",
    "Como retrieval corpus (o proprietary knowledge base), usaremos los libros en Inglés de Harry Potter, con un cambio fundamental, los nombres de los personajes estarán modificados.\n",
    "\n",
    "Los libros de harry potter están en `data/practica_rag/harry_potter_books`\n",
    "Además se encuentra una trivia en `data/practica_rag/harry_potter_trivia`\n",
    "\n",
    "En ambos directorios se han aplicados los scripts de `TP2/harrypotter/name_replacement.py` para generar los archivos con los nombres modificados (que se encuentran en subdirectorios `data/practica_rag/harry_potter_books/_modified/` y `data/practica_rag/harry_potter_trivia/_modified/`).\n",
    "\n",
    "### Calculando los embeddings\n",
    "\n",
    "Se ha ejecutado el script `TP2/harrypotter/generate_embeddings.py` para generar los embeddings que han sido guardados en `data/practica_rag/embeddings.tar.xz` (al ser muy pesados han sido subidos de forma comprimida.\n",
    "\n",
    "### Ejecutando el RAG\n",
    "\n",
    "El RAG simplemente consiste, como se dijo, en:\n",
    "- Calcular los párrafos relevantes. Para ello calculamos el embedding de la consulta y buscamos lo párrafos con embeddings de mayor similitud.\n",
    "- Incluirlos en el contexto del prompt e instruir al modelo para que responda teniendo en cuenta el contexto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dir = '../data/practica_rag/embeddings/'\n",
    "embeddings = np.load(os.path.join(embeddings_dir, \"embeddings.npy\"))\n",
    "texts = np.load(os.path.join(embeddings_dir, \"texts.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "def search_embeddings(query, top_k=5):\n",
    "    query_embedding = get_embedding(query)\n",
    "    similarities = np.array([cosine_similarity(query_embedding, emb) for emb in embeddings])\n",
    "    top_k_indices = similarities.argsort()[-top_k:][::1]\n",
    "    results = [texts[i] for i in top_k_indices[::-1]]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query GPT with search results as context\n",
    "def query_gpt_with_context(query, top_k=10):\n",
    "    context_results = search_embeddings(query, top_k=top_k)\n",
    "    context = \"\\n\\n\".join(context_results)\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    print(f\"This is the full prompt with the context of selected fragments:\\n\\n {prompt}\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"Answer the question based on the provided context.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reemplacemos ahora los nombres en una lista de preguntas de Harry Potter, y pongamos a prueba nuestro RAG!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the full prompt with the context of selected fragments:\n",
      "\n",
      " Context:\n",
      "Villalba was standing outside his hut, one hand on the collar of his enormous black boarhound, Fang. There were several open wooden crates on the ground at his feet, and Fang was whimpering and straining at his collar, apparently keen to investigate the contents more closely. As they drew nearer, an odd rattling noise reached their ears, punctuated by what sounded like minor explosions.\n",
      "\n",
      "Roberto, Diego, and Carolina had always known that Villalba had an unfortunate liking for large and monstrous creatures. During their first year at Hogwarts he had tried to raise a dragon in his little wooden house, and it would be a long time before they forgot the giant, three- headed dog he'd christened \"Fluffy.\" And if, as a boy, Villalba had heard that a monster was hidden somewhere in the castle, Roberto was sure he'd have gone to any lengths for a glimpse of it. He'd probably thought\n",
      "\n",
      "Villalba was sitting in his shirtsleeves at his scrubbed wooden table; his boarhound, Fang, had his head in Villalba's lap. One look told them that Villalba had been drinking a lot; there was a pewter tankard almost as big as a bucket in front of him, and he seemed to be having difficulty getting them into focus.\n",
      "　　\"'Spect it's a record,\" he said thickly, when he recognized them. \"Don' reckon they've ever had a teacher who lasted on'y a day before.\"\n",
      "\n",
      "Roberto and Diego grinned at Villalba, who gave them a furtive smile from behind his bushy beard. Villalba would have liked nothing better than a pet dragon, as Roberto, Diego, and Carolina knew only too well - he had owned one for a brief period during their first year, a vicious Norwegian Ridgeback by the name of Norbert. Villalba simply loved monstrous creatures, the more lethal, the better.\n",
      "\n",
      "\"I found out something about him,\" he told Villalba. \"He tried to get past that three-headed dog on Halloween. It bit him. We think he was trying to steal whatever it's guarding.\"\n",
      "Villalba dropped the teapot.\n",
      "\"How do you know about Fluffy?\" he said.\n",
      "153\n",
      "\"Fluffy?\"\n",
      "\"Yeah -- he's mine -- bought him off a Greek chappie I met in the pub las' year -- I lent him to Dominguez to guard the\n",
      "\"Yes?\" said Roberto eagerly.\n",
      "\"Now, don't ask me anymore,\" said Villalba gruffly. \"That's top secret, that is.\"\n",
      "\n",
      "Villalba bit his lip.\n",
      "188\n",
      "\"I -- I know I can't keep him forever, but I can't jus' dump him, I can't.\"\n",
      "Roberto suddenly turned to Diego. Charlie, he said.\n",
      "\"You're losing it, too,\" said Diego. \"I'm Diego, remember?\"\n",
      "\"No -- Charlie -- your brother, Charlie. In Romania. Studying dragons. We could send Norbert to him. Charlie can take care of him and then put him back in the wild!\"\n",
      "\"Brilliant!\" said Diego. \"How about it, Villalba?\"\n",
      "\n",
      "In a bizarre twist, Villalba is reputed to have developed a close friendship with the boy who brought around You-Know-Who's fall from power - thereby driving Villalba's own mother, like the rest of You-Know-Who's supporters, into hiding. Perhaps Roberto Gonzales is unaware of the unpleasant truth about his large friend - but Alfonso Dominguez surely has a duty to ensure that Roberto Gonzales, along with his fellow students, is warned about the dangers of associating with part-giants.\n",
      "\n",
      "'Villalba, it's us!' Roberto called through the keyhole.\n",
      "    'Shoulda known!' said a gruff voice.\n",
      "    They beamed at each other under the Cloak; they could tell by Villalba's voice that he was pleased. 'Bin home three seconds . . . out the way, Fang . . . out the way, yeh dozy dog . . .'\n",
      "    The bolt was drawn back, the door creaked open and Villalba's head appeared in the gap.\n",
      "    Carolina screamed.\n",
      "\n",
      "\"Villalba makes six,\" Luis pointed out.\n",
      "\"Oh . . . yes. ..\" said Madam Pomfrey, who seemed to have been counting Villalba as several people due to his vastness. To cover her confusion, she hurried off to clear up his muddy foot prints with her wand.\n",
      "\"I don' believe this,\" said Villalba hoarsely, shaking his great shaggy head as he stared down at Diego. \"Jus' don' believe it... Look at him lyin' there. . . . Who'd want ter hurt him, eh?\"\n",
      "\n",
      "Villalba’s hut loomed out of the darkness. There were no lights, no sound of Fang \n",
      "scrabbling at the door, his bark booming in welcome. All those visits to Villalba, and the \n",
      "gleam of the copper kettle on the fire, and rock cakes and giant grubs, and his great \n",
      "bearded face, and Diego vomiting slugs, and Carolina helping him save Norbert . . . \n",
      "\n",
      " He moved on, and now he reached the edge of the forest, and he stopped.\n",
      "\n",
      "Question: What is the name of Villalba's faithful companion and dog\n",
      "\n",
      "Answer:\n",
      "Answer: Villalba's faithful companion and dog is named Fang.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the name of Villalba's faithful companion and dog\"\n",
    "answer = query_gpt_with_context(query)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio C.1: \"Evaluando nuestro RAG\" (opcional)\n",
    "\n",
    "Utilizar el código provisto para responder la trivia provista de forma automática, de manera que se compute una métrica de accuracy. \n",
    "\n",
    "Tener en cuenta que como hay varias maneras de expresar una respuesta correcta, no es correcto utilizar la comparación por igualdad.\n",
    "\n",
    "#### Ejercicio C.2: \"Reportando y analizando errores\" (opcional)\n",
    "\n",
    "Reportar 5 preguntas donde funcione mal. Tratar de determinar por qué (el retrival no encontró párrafos relevantes o el modelo se confunde al generar la respuesta?).\n",
    "\n",
    "#### Ejercicio C.3: \"Optimizando nuestro RAG\" (opcional)\n",
    "\n",
    "Probar variantes como incrementar el contexto, modificar el prompt o samplear varias veces la query al LLM y combinarlas de alguna forma y reportar si mejoran los resultados.\n",
    "\n",
    "#### Ejercicio C.4: \"Diseñando nuestro propio RAG\" (opcional)\n",
    "\n",
    "Diseñar tu propio sistema de RAG para un dataset propio.\n",
    "Diseñar un dataset de preguntas y respuestas para evaluarlo.\n",
    "Probar la accuracy del modelo sin retrival y con retrival. Si la data es pública, el modelo sin retrival debería funcionar razonablemente bien."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
