{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pycoingecko import CoinGeckoAPI\n",
    "import requests\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = ''\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7VwQRulwOya"
   },
   "source": [
    "# TP 2 - Parte B: Chatbots y Agentes\n",
    "\n",
    "## Crear nuestro propio chatbot\n",
    "\n",
    "En la \"Parte A\" vimos como hacer una llamada a modelos conversacionales (que responden a nuestras instrucciones, como ChatGPT). Vimos que pueden ser usados para resolver varios problemas de NLP.\n",
    "\n",
    "Y vimos como crear aplicaciones útiles, cómo el whatsappanalyzer. Muchas veces requiriendo no una, sino muchas llamadas al modelo y combinándolas: usando el texto de alguna llamada anterior para construir el prompt.\n",
    "\n",
    "**Nota:** notar que por prompt nos referimos a toda la conversación que le pasamos a la API call en el campo messages.\n",
    "\n",
    "Crear un chatbot no es más que una aplicación más que podemos hacer, pero que merece atención especial ya que es una aplicación frecuentemente requerida.\n",
    "\n",
    "A continaución está el código de un chatbot que programé haciendo llamadas a las API de ChatGPT.\n",
    "\n",
    "#### Ejercicio B.1\n",
    "\n",
    "A continuación hay un chatbot que he programado pero tiene un error. Debido al error el chatbot no tiene memoria: no recuerda qué se conversó en mensajes anteriores. Modificalo de modo que tenga memoria.\n",
    "Una forma de corroborar que la memoria está correctamente implementada es darle un dato fáctico y preguntarle por el mismo a continuación.Por ejemplo probá decirle tu nombre y preguntale en un mensaje sucesivo cómo te llamás. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the OpenAI Chat! Type 'exit' to end the chat.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  hola me llamo mariano\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: ¡Hola Mariano! ¿En qué puedo ayudarte hoy?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  como me llamo?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: No tengo la capacidad de saber tu nombre, ya que soy un asistente virtual y no tengo acceso a esa información. ¿En qué más puedo ayudarte?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome to the OpenAI Chat! Type 'exit' to end the chat.\\n\")\n",
    "\n",
    "while True:\n",
    "        history = []\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower().strip() == \"exit\":\n",
    "            break\n",
    "            \n",
    "        history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "          model=\"gpt-3.5-turbo\",\n",
    "          messages=history\n",
    "        )\n",
    "\n",
    "        assistant_message = response.choices[0].message.content.strip()\n",
    "        print(f\"Assistant: {assistant_message}\\n\")\n",
    "        history.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "\n",
    "print(\"Chat ended.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentes\n",
    "\n",
    "Los agentes en IA son un concepto bastante viejo, del siglo pasado. Muy anterior a la existencia de LLMs.\n",
    "\n",
    "Conceptualmente, son sistemas que interactúan con el ambiente, produciendo acciones en el mismo.\n",
    "\n",
    "![alt text](agents-in-ai.png \"Agent\")\n",
    "\n",
    "El concepto central del machine learning actual es el de modelo. El modelo es un sistema que hace predicciones. Notar que si bien un modelo puede ser parte de un agente son conceptualmente distintos. Mientras que el modelo quiere predecir algo, el agente tiene además la capacidad de actuar: la relación entre la predicción del modelo y la acción llevada a cabo por el agente no es trivial y tiene que ser estudiada con cuidado.\n",
    "\n",
    "#### Prediciendo bien, actuando mal\n",
    "\n",
    "Como anécdota ilustrativa para marcar esta diferencia conceptual está el caso de una startup de real estate que entrenó un modelo para predecir el precio de propiedades. \n",
    "\n",
    "Si bien conocían el márgen de error promedio, luego cuando lo usaron para tomar decisión de qué inmuebles comprar y a qué precio terminaron teniendo grandes pérdidas.\n",
    "\n",
    "Lo primero fue pensar que tenían un error en la predicción o que el error era más grande del que habían calculado.\n",
    "\n",
    "El análisis post-mortem arrojó que el problema había sido otro: ellos ofertaban un precio de compra que, en promedio, tenía un error como el calculado. Los vendedores, que poseían más información acerca de sus inmuebles, aceptaban esta oferta cuando el precio les era conveniente y la rechazaban cuando no. O sea, que si bien el error era promedio las compras sólo se efectuaban en casos donde la casa había sido tasada en ventaja para el vendedor, generando una desventaja sistemática para la compañía.\n",
    "\n",
    "## Agentes y LLMs\n",
    "\n",
    "Vimos que los modelos conversacionales responden a nuestras instrucciones con precisión. Entonces pueden ser usados como el \"corazón\" de los agentes: le podemos dar instrucciones para que, dado un escenario y un conjunto de acciones decidan qué acción aplicar. Estas acciones repercutirán en el escenario, y luego le presentaremos un nuevo escenario y otro conjuntos de acciones.\n",
    "\n",
    "Es un concepto bastante amplio. así que lo mejor es verlo con un ejemplo. \n",
    "Ahora imaginemos que queremos agregarle a nuestro chat anterior la capacidad de contestar el clima que hace en cierta ciudad.\n",
    "Entonces crearemos un sistema que decida que acción realizar:\n",
    "\n",
    "- Responderle al usuario con cierto texto.\n",
    "- Consultar al servicio meteorológico de cierta ciudad.\n",
    "\n",
    "A veces la respuesta de los LLMs no es robusta: le pedimos que responda con un JSON y nos da una string que no parsea como JSON o le pedimos que ciertos campos estén presentes y no lo están. El código que maneje la respuesta, la parsee y la traduzca en acciones debiera manejar este tipo de errores para que la aplicación no crashee cuando sucedan. E\n",
    "\n",
    "vitarlos a nivel LLM es un área de investigación activa y un problema no resuelto, en nuestro agente (más abajo) incluímos como último mensaje la consigna que dice como debiera ser el formato de la respuesta (por más que también fue incluído como primer mensaje). Algunos resultados prematuros arrojan que los LLM prestan más atención al último mensaje y en conversaciones largas \"olvidan\" los primeros.\n",
    "\n",
    "#### Ejercicio B.2\n",
    "\n",
    "Entender el código del chat, leer la conversación ejemplo que se incluye en esta notebook.\n",
    "Agregar una ciudad nueva y usarlo para preguntarle el clima en esa ciudad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the OpenAI Chat! Type 'exit' to end the chat.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  Hola como estas?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Hola! Estoy aquí para ayudarte. ¿En qué puedo asistirte hoy?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  Bien, mi nombre es Mariano, estas listo para las preguntas?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: ¡Hola Mariano! Estoy listo para responder tus preguntas. Adelante, ¿en qué puedo ayudarte?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  ok como me llamo?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Tu nombre es Mariano. ¿Hay algo más en lo que pueda ayudarte?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  Bien. Como esta el clima en la ciudad de rosario?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: El clima en la ciudad de Rosario está nublado en este momento.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  Que bien que funciona esto! Y en la capital del pais?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: El clima en la ciudad de Rosario está nublado en este momento.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  Y en la capital del pais donde rosario esta ubicada?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Es posible que haya un problema temporal con la información del clima para Buenos Aires. Te recomendaría intentarlo más tarde o consultar otro servicio meteorológico. ¿Puedo ayudarte con algo más?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  Y en la capital de la provincia donde rosario esta ubicada?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: La capital de la provincia donde se encuentra Rosario, Santa Fe, tiene un clima soleado en este momento.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome to the OpenAI Chat! Type 'exit' to end the chat.\\n\")\n",
    "import json\n",
    "instruction_prompt = \"\"\"\n",
    "REMINDER ABOUT THE FORMAT OF THE ANSWER:\n",
    "You are an agent that can decide to:\n",
    "- reply to the user.\n",
    "- query the weather service.\n",
    "\n",
    "You are going to be provided with the message history with an user. As a result you must produce a JSON with the fields:\n",
    "- response_type (manadatory field): 'reply_to_user' | 'query_weather' if you want to reply to the user or query the weather service.\n",
    "- city (only if response_type is 'query_weather'): str the name of the city to ask to the weather service.\n",
    "- response (only if response_type is 'reply_to_user'): str the response to the user.\n",
    "\"\"\".strip()\n",
    "weather_service_api = {'rosario': 'cloudy', 'santa fe': 'sunny'}\n",
    "history = [{\"role\": \"system\", \"content\": instruction_prompt}]\n",
    "skip_input = False\n",
    "while True:\n",
    "    if not skip_input:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower().strip() == \"exit\":\n",
    "            break\n",
    "\n",
    "        history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    skip_input = False\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=history+[{\"role\": \"user\", \"content\": instruction_prompt}]\n",
    "    )\n",
    "\n",
    "    agent_response_raw = response.choices[0].message.content.strip()\n",
    "    #print(agent_response_raw)\n",
    "    agent_response_json = json.loads(agent_response_raw)\n",
    "    if agent_response_json['response_type'] == 'reply_to_user':\n",
    "        print(f\"Assistant: {agent_response_json['response']}\\n\")\n",
    "        history.append({\"role\": \"assistant\", \"content\": agent_response_json['response']})\n",
    "    elif agent_response_json['response_type'] == 'query_weather':\n",
    "        city = agent_response_json['city'].lower().strip()\n",
    "        if city in weather_service_api:\n",
    "            weather = weather_service_api[city]\n",
    "            extra_info_added = f\"(The system weather shows that the weather for the cit {city} is {weather})\"\n",
    "        else:\n",
    "            extra_info_added = f\"(The system weather shows that the weather for the cit {city} is unavailable, please do not ask again for this city)\"\n",
    "        history.append({\"role\": \"user\", \"content\": extra_info_added})\n",
    "        skip_input = True\n",
    "        \n",
    "\n",
    "print(\"Chat ended.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio B.3\n",
    "\n",
    "Hacer un caso de uso de agente siguiendo el ejemplo anterior como guía.\n",
    "\n",
    "## Herramientas (tools)\n",
    "\n",
    "Estas acciones desencadenan, como en ejemplo anterior llamadas a servicios externos (como weather_service_api, como podría ser consultas a base de datos, a una calculadora, etc), es por ello que se llaman herramientas. Ya que permiten al modelo usar cierto servicio externo como herramienta.\n",
    "\n",
    "Una manera de que el modelo entienda las acciones que tiene disponibles y el formato para llamarlas es incluir casos de uso de acciones durante el entrenamiento. Los proveedores de servicios como chatgtp permiten conectar los modelos con herramientas, incluyendo en su API un campo para especificarle el formato con el que el modelo debe llamar a la herramienta (sus campos y la semántica de los campos y resultados). Notar que igual que pasaba en el ejemplo anterior: el que hace la llamda al modelo (o sea, nosotros) es el responsable de implementar las llamadas a las herramientas. El modelo solo dirá \"quiero llamar a la herramienta X con los argumentos A, B, C\".\n",
    "\n",
    "Las API también incluyen una manera de incluir respuestas a llamadas a herramientas del modelo. De modo que si el modelo nos contesta diciendo que necesita usar get_weather_service para la ciudad 'rosario', el siguiente request además de todo el historial le contestará \"la respuesta a la función get_weather-service que hiciste en tu solicitud anterior es ...\".\n",
    "\n",
    "A continuación implementamos dos funciones que luego le daremos a nuestro asistente:\n",
    "- Una que permite conocer las condiciones del clima en cierta ciudad.\n",
    "- Otra que permite buscar datos en tiempo real de cotizaciones de criptomonedas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1qmyfnks10_1",
    "outputId": "8e7269ce-4227-4e59-f345-6976a5fb3200"
   },
   "source": [
    "** Llamadas a funciones**     \n",
    "\n",
    "Las llamadas a funciones de GPT permiten \n",
    "\n",
    "**Ejercicio opcional:** Siguiendo el ejemplo brindado, crear tu propias funciones para potenciar al asistente.\n",
    "\n",
    "**Ejemplo:** Escribir funciones para:\n",
    "- Permitirle al asistente obtener datos en tiempo real de las condiciones climatológicas en una ubicación determinada. El Asistente deberá poder responder acertadamente a consultas como \"¿Cuál es actualmente la temperatura en México DF? Detallar otras condiciones climatológicas de ser posible\"\n",
    "- Permitirle al asistente buscar datos en tiempo real de cotizaciones de criptomonedas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "KAeQsq1lzKsq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'city': 'Mexico',\n",
       " 'temperature': 25.84,\n",
       " 'description': 'Clouds',\n",
       " 'humidity': 96,\n",
       " 'pressure': 1010}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_weather(city):\n",
    "    api_key = '7663c30fc4c275ea5f373e7052d8a3d3'\n",
    "    url = f'http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric&lang=es'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        weather = {\n",
    "            'city': data['name'],\n",
    "            'temperature': data['main']['temp'],\n",
    "            'description': data['weather'][0]['main'],\n",
    "            'humidity': data['main']['humidity'],\n",
    "            'pressure': data['main']['pressure']\n",
    "        }\n",
    "        return weather\n",
    "    else:\n",
    "        return {'error': 'Weather data could not be found for the specified city.'}\n",
    "\n",
    "get_weather('Mexico')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "tgdMKkpgaBlY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'currency': 'Bitcoin', 'price_usd': 76401}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_crypto_price(currency):\n",
    "    cg = CoinGeckoAPI()\n",
    "    data = cg.get_price(ids=currency.lower(), vs_currencies='usd')\n",
    "    if data:\n",
    "        price = data.get(currency.lower(), {}).get('usd')\n",
    "        if price:\n",
    "            return {'currency': currency.capitalize(), 'price_usd': price}\n",
    "    return {'error': 'Cound not find price for the specified cryptocurrency.'}\n",
    "\n",
    "get_crypto_price('bitcoin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WHmRLUdrOWEm",
    "outputId": "0b3008b8-010b-469b-bc5b-9ab0fca50f3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the OpenAI Chat with Function Calling! Type 'exit' to end the chat.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  which is the wather of mexico df and price of bitcoin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_Zyq6yAjbIwozSoNdxwwf03vk', function=Function(arguments='{\"city\": \"Mexico City\"}', name='get_weather'), type='function'), ChatCompletionMessageToolCall(id='call_t2TXIFsrFh50YFAiLrt303b5', function=Function(arguments='{\"currency\": \"bitcoin\"}', name='get_crypto_price'), type='function')], refusal=None)\n",
      "\n",
      "Assistant is calling function: get_weather with arguments {'city': 'Mexico City'}\n",
      "\n",
      "\n",
      "Assistant is calling function: get_crypto_price with arguments {'currency': 'bitcoin'}\n",
      "\n",
      "Assistant: The weather in Mexico City is currently cloudy with a temperature of 24.97°C, humidity of 30%, and a pressure of 1013 hPa. \n",
      "\n",
      "The price of Bitcoin is $76,401 USD.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  What's the last message?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='The last message was:\\n\"The price of Bitcoin is $76,401 USD.\"', role='assistant', function_call=None, tool_calls=None, refusal=None)\n",
      "Assistant: The last message was:\n",
      "\"The price of Bitcoin is $76,401 USD.\"\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "debug_mode = False\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Gets the current weather of a specific city.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"city\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Name of the city, for example, 'Mexico City'.\"\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"city\"],\n",
    "                \"additionalProperties\": False,\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_crypto_price\",\n",
    "            \"description\": \"Gets the current price in USD of a specific cryptocurrency.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"currency\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Name of the cryptocurrency, for example, 'bitcoin', 'ethereum'.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"currency\"],\n",
    "                \"additionalProperties\": False,\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "messages = []\n",
    "\n",
    "print(\"Welcome to the OpenAI Chat with Function Calling! Type 'exit' to end the chat.\\n\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Chat ended.\")\n",
    "            break\n",
    "\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",  # Use a model that supports function calling\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "        )\n",
    "\n",
    "        # Extract the assistant's message\n",
    "        assistant_message = response.choices[0].message\n",
    "\n",
    "        if debug_mode:\n",
    "            print(assistant_message)\n",
    "\n",
    "        # Check if the assistant wants to call a function\n",
    "        if assistant_message.tool_calls:\n",
    "            messages.append(assistant_message)  # Assistant's function call\n",
    "            \n",
    "            for tool_call in assistant_message.tool_calls:\n",
    "                function_args = json.loads(tool_call.function.arguments)\n",
    "                function_name = tool_call.function.name\n",
    "    \n",
    "                if debug_mode:\n",
    "                    print(f\"\\nAssistant is calling function: {function_name} with arguments {function_args}\\n\")\n",
    "    \n",
    "                # Call the appropriate function based on the function name\n",
    "                if function_name == \"get_weather\":\n",
    "                    function_response = get_weather(**function_args)\n",
    "                elif function_name == \"get_crypto_price\":\n",
    "                    function_response = get_crypto_price(**function_args)\n",
    "                else:\n",
    "                    function_response = {\"error\": \"Function not recognized.\"}\n",
    "    \n",
    "                # Convert the function response to a JSON-formatted string\n",
    "                function_response_json = json.dumps(function_response)\n",
    "\n",
    "                function_call_result_message = {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"content\": function_response_json,\n",
    "                    \"tool_call_id\": tool_call.id\n",
    "                }\n",
    "                messages.append(function_call_result_message)\n",
    "\n",
    "            # Get the assistant's final response using the function's output\n",
    "            second_response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=messages\n",
    "            )\n",
    "\n",
    "            # Extract and print the assistant's final answer\n",
    "            final_answer = second_response.choices[0].message.content\n",
    "            print(f\"Assistant: {final_answer}\\n\")\n",
    "\n",
    "            # Append the final answer to the conversation\n",
    "            messages.append({\"role\": \"assistant\", \"content\": final_answer})\n",
    "        else:\n",
    "            # If no function call, just print the assistant's response\n",
    "            assistant_content = assistant_message.content.strip()\n",
    "            print(f\"Assistant: {assistant_content}\\n\")\n",
    "            messages.append(assistant_message)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio B.4\n",
    "\n",
    "Implementá tu propio agente. Notá que las tools no necesariamente tienen que llamar a la API. Pueden:\n",
    "- Hacer una cuenta matemática que vos conozcas.\n",
    "- Ejecutar un algoritmo para modificar el entorno.\n",
    "- Las funciones disponibles pueden depender del entorno (dejar de estar disponibles / pasara estar disponibles).\n",
    "\n",
    "Por ejemplo, podríamos crear un laberinto con puertas y llaves y el modelo podría tener como acciones dar un paso en ese laberinto en alguna dirección, recoger una llamer, usar una llave que tenga en el inventario, etc. Definitivamente la idea sería muy distinto al ejemplo de arriba pero el concepto es el mismo: llamar repetidas veces a una LLM y usar la respuesta del mismo para modificar el entorno y consecuentemente la siguiente llamada.\n",
    "\n",
    "No te limites: el corazón de esto es hacer varias llamadas a LLMs y usar la respuesta del mismo para modificar el entorno y consecuentemente la siguiente llamada. No es necesario que el agente que hagas funcione o que sea útil. La capacidad de razocinio de los LLM es sorprendente pero a veces pueden ser sorprendentemente estúpidos y todavía conocer sus limitaciones es un área de investigación activa donde hay pocas luces."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
