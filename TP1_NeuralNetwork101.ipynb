{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7047fa1-fa53-4496-b9cc-db8b410c58a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df203554-56b6-4c5f-ac16-6f4be578e3fc",
   "metadata": {},
   "source": [
    "# Introducción a redes neuronales en pytorch\n",
    "\n",
    "En este notebook veremos como aprovechar la diferenciación automática que provee Pytorch (autograd) para programar redes neuronales.\n",
    "\n",
    "Lo haremos para identificar dígitos (número del 0-9) dada una imagen del mismo. \n",
    "\n",
    "El dataset es conocido como MNIST. La idea NO es aprender a reconocer dígitos (ni imágenes) sino fijar los conceptos de redes neuronales y ver como se implementan en Pytorch. Además conocer práctica habituales a la hora de entrenar redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11632e7a-c71c-44dc-848c-92544b0ea438",
   "metadata": {},
   "source": [
    "### Descargando el dataset\n",
    "\n",
    "Descargaremos el dataset MNIST de la página de pytorch. Para ello crearemos un subdirectorio en nuestra carpeta actual llamado \"data\" donde guardaremos nuestros datasets (esta es una práctica habitual en los proyectos de Machine Learning). Dentro de la carpeta \"data\" crearemos una subcarpeta para cada dataset. En este caso crearemos la subcarpeta \"mnist\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47b7ab9f-8339-4eb2-a20b-62b2ae65dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "# Creara ./data/mnist si no existe \n",
    "# (si no existe ./data lo creará y creará ./data/mnist)\n",
    "# (y si existe pero no existe ./data/mnist sólo creará este último).\n",
    "# Cualquier cuestión que no se entienda, consultar la documentación de las librerías\n",
    "# por ejemplo https://docs.python.org/3/library/pathlib.html\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"https://github.com/pytorch/tutorials/raw/main/_static/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "# Descargamos mnist.pkl.gz utilizando un HTTP GET request\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01be179-f2a7-4bbe-a825-6b269c18327f",
   "metadata": {},
   "source": [
    "### Cargando el dataset\n",
    "\n",
    "Cada dataset tiene diferentes formas. Hay veces que se presenta como un conjunto de imágenes dentro de carpetas cuyo nombre indica la clase a la que pertenece. En este caso el dataset MNIST descargado contiene 4 numpy arrays con los datos de las imágenes y las clases de train y tests respectivamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d3bcb5b-7c81-4ef4-988d-dcf4eda1b624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# Los arrays de las imagenes fueron guardados en un archivo formato pickle, que se utiliza para persistir variable en Python\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e5500e3-d134-472e-a0d2-6c738f7ab831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 784), (50000,), (10000, 784), (10000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebddc0a4-bec0-4b51-b491-84d62ee44e4c",
   "metadata": {},
   "source": [
    "#### Visualizando el dataset\n",
    "\n",
    "Visualizemos la clase y la imagen del primer elemento del training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae0b7336-a2f1-4d2b-8879-3addd06bc657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8c344d-b598-4bc4-a36f-4c0d939d1738",
   "metadata": {},
   "source": [
    "x_train contiene el valor de los $28 \\times 28$ pixeles \"aplanado\" (transformado en una sóla dimensión de longitud 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "615a2825-9f56-45ea-bb63-663ae5c9a46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01171875, 0.0703125 , 0.0703125 ,\n",
       "       0.0703125 , 0.4921875 , 0.53125   , 0.68359375, 0.1015625 ,\n",
       "       0.6484375 , 0.99609375, 0.96484375, 0.49609375, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.1171875 , 0.140625  , 0.3671875 , 0.6015625 ,\n",
       "       0.6640625 , 0.98828125, 0.98828125, 0.98828125, 0.98828125,\n",
       "       0.98828125, 0.87890625, 0.671875  , 0.98828125, 0.9453125 ,\n",
       "       0.76171875, 0.25      , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.19140625, 0.9296875 ,\n",
       "       0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125,\n",
       "       0.98828125, 0.98828125, 0.98828125, 0.98046875, 0.36328125,\n",
       "       0.3203125 , 0.3203125 , 0.21875   , 0.15234375, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.0703125 , 0.85546875, 0.98828125, 0.98828125,\n",
       "       0.98828125, 0.98828125, 0.98828125, 0.7734375 , 0.7109375 ,\n",
       "       0.96484375, 0.94140625, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.3125    , 0.609375  , 0.41796875, 0.98828125, 0.98828125,\n",
       "       0.80078125, 0.04296875, 0.        , 0.16796875, 0.6015625 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.0546875 ,\n",
       "       0.00390625, 0.6015625 , 0.98828125, 0.3515625 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.54296875,\n",
       "       0.98828125, 0.7421875 , 0.0078125 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04296875, 0.7421875 , 0.98828125,\n",
       "       0.2734375 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.13671875, 0.94140625, 0.87890625, 0.625     ,\n",
       "       0.421875  , 0.00390625, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31640625, 0.9375    , 0.98828125, 0.98828125, 0.46484375,\n",
       "       0.09765625, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.17578125,\n",
       "       0.7265625 , 0.98828125, 0.98828125, 0.5859375 , 0.10546875,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.0625    , 0.36328125,\n",
       "       0.984375  , 0.98828125, 0.73046875, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.97265625, 0.98828125,\n",
       "       0.97265625, 0.25      , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.1796875 , 0.5078125 ,\n",
       "       0.71484375, 0.98828125, 0.98828125, 0.80859375, 0.0078125 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15234375,\n",
       "       0.578125  , 0.89453125, 0.98828125, 0.98828125, 0.98828125,\n",
       "       0.9765625 , 0.7109375 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.09375   , 0.4453125 , 0.86328125, 0.98828125, 0.98828125,\n",
       "       0.98828125, 0.98828125, 0.78515625, 0.3046875 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.08984375, 0.2578125 , 0.83203125, 0.98828125,\n",
       "       0.98828125, 0.98828125, 0.98828125, 0.7734375 , 0.31640625,\n",
       "       0.0078125 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.0703125 , 0.66796875, 0.85546875,\n",
       "       0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.76171875,\n",
       "       0.3125    , 0.03515625, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21484375, 0.671875  ,\n",
       "       0.8828125 , 0.98828125, 0.98828125, 0.98828125, 0.98828125,\n",
       "       0.953125  , 0.51953125, 0.04296875, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.53125   , 0.98828125, 0.98828125, 0.98828125,\n",
       "       0.828125  , 0.52734375, 0.515625  , 0.0625    , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3890ee9-7efc-4ce1-b79e-00985c17964d",
   "metadata": {},
   "source": [
    "Para visualizar la imagen tenemos que volver al array de pixeles a su estado original (dos dimensiones de tamaño 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f24a8e11-f297-4558-b1ad-35f523645f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbe0lEQVR4nO3df2xV9f3H8dflR6+I7e1KbW8rPyygsIlgxqDrVMRRKd1G5McWdS7BzWhwrRGYuNRM0W2uDqczbEz5Y4GxCSjJgEEWNi22ZLNgQBgxbg0l3VpGWyZb7y2FFmw/3z+I98uVFjyXe/u+vTwfySeh955378fjtU9vezn1OeecAADoZ4OsNwAAuDIRIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYGKI9QY+qaenR8eOHVN6erp8Pp/1dgAAHjnn1N7ervz8fA0a1PfrnKQL0LFjxzRq1CjrbQAALlNTU5NGjhzZ5/1J9y249PR06y0AAOLgUl/PExag1atX6/rrr9dVV12lwsJCvfvuu59qjm+7AUBquNTX84QE6PXXX9eyZcu0YsUKvffee5oyZYpKSkp0/PjxRDwcAGAgcgkwffp0V1ZWFvm4u7vb5efnu8rKykvOhkIhJ4nFYrFYA3yFQqGLfr2P+yugM2fOaP/+/SouLo7cNmjQIBUXF6u2tvaC47u6uhQOh6MWACD1xT1AH374obq7u5Wbmxt1e25urlpaWi44vrKyUoFAILJ4BxwAXBnM3wVXUVGhUCgUWU1NTdZbAgD0g7j/PaDs7GwNHjxYra2tUbe3trYqGAxecLzf75ff74/3NgAASS7ur4DS0tI0depUVVVVRW7r6elRVVWVioqK4v1wAIABKiFXQli2bJkWLVqkL3zhC5o+fbpefvlldXR06Nvf/nYiHg4AMAAlJED33HOP/vOf/+jpp59WS0uLbrnlFu3cufOCNyYAAK5cPuecs97E+cLhsAKBgPU2AACXKRQKKSMjo8/7zd8FBwC4MhEgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmhlhvAEgmgwcP9jwTCAQSsJP4KC8vj2nu6quv9jwzYcIEzzNlZWWeZ372s595nrnvvvs8z0hSZ2en55nnn3/e88yzzz7reSYV8AoIAGCCAAEATMQ9QM8884x8Pl/UmjhxYrwfBgAwwCXkZ0A33XST3nrrrf9/kCH8qAkAEC0hZRgyZIiCwWAiPjUAIEUk5GdAhw8fVn5+vsaOHav7779fjY2NfR7b1dWlcDgctQAAqS/uASosLNS6deu0c+dOvfLKK2poaNDtt9+u9vb2Xo+vrKxUIBCIrFGjRsV7SwCAJBT3AJWWluob3/iGJk+erJKSEv3xj39UW1ub3njjjV6Pr6ioUCgUiqympqZ4bwkAkIQS/u6AzMxM3Xjjjaqvr+/1fr/fL7/fn+htAACSTML/HtDJkyd15MgR5eXlJfqhAAADSNwD9Pjjj6umpkb//Oc/9c4772j+/PkaPHhwzJfCAACkprh/C+7o0aO67777dOLECV177bW67bbbtGfPHl177bXxfigAwAAW9wBt2rQp3p8SSWr06NGeZ9LS0jzPfOlLX/I8c9ttt3mekc79zNKrhQsXxvRYqebo0aOeZ1atWuV5Zv78+Z5n+noX7qX87W9/8zxTU1MT02NdibgWHADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgwuecc9abOF84HFYgELDexhXllltuiWlu165dnmf4dzsw9PT0eJ75zne+43nm5MmTnmdi0dzcHNPc//73P88zdXV1MT1WKgqFQsrIyOjzfl4BAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwMQQ6w3AXmNjY0xzJ06c8DzD1bDP2bt3r+eZtrY2zzN33nmn5xlJOnPmjOeZ3/72tzE9Fq5cvAICAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAExwMVLov//9b0xzy5cv9zzzta99zfPMgQMHPM+sWrXK80ysDh486Hnmrrvu8jzT0dHheeamm27yPCNJjz32WExzgBe8AgIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATPicc856E+cLh8MKBALW20CCZGRkeJ5pb2/3PLNmzRrPM5L04IMPep751re+5Xlm48aNnmeAgSYUCl30v3leAQEATBAgAIAJzwHavXu35s6dq/z8fPl8Pm3dujXqfuecnn76aeXl5WnYsGEqLi7W4cOH47VfAECK8Bygjo4OTZkyRatXr+71/pUrV2rVqlV69dVXtXfvXg0fPlwlJSXq7Oy87M0CAFKH59+IWlpaqtLS0l7vc87p5Zdf1g9+8APdfffdkqT169crNzdXW7du1b333nt5uwUApIy4/gyooaFBLS0tKi4ujtwWCARUWFio2traXme6uroUDoejFgAg9cU1QC0tLZKk3NzcqNtzc3Mj931SZWWlAoFAZI0aNSqeWwIAJCnzd8FVVFQoFApFVlNTk/WWAAD9IK4BCgaDkqTW1tao21tbWyP3fZLf71dGRkbUAgCkvrgGqKCgQMFgUFVVVZHbwuGw9u7dq6Kiong+FABggPP8LriTJ0+qvr4+8nFDQ4MOHjyorKwsjR49WkuWLNGPf/xj3XDDDSooKNBTTz2l/Px8zZs3L577BgAMcJ4DtG/fPt15552Rj5ctWyZJWrRokdatW6cnnnhCHR0devjhh9XW1qbbbrtNO3fu1FVXXRW/XQMABjwuRoqU9MILL8Q09/H/UHlRU1Pjeeb8v6rwafX09HieASxxMVIAQFIiQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACa6GjZQ0fPjwmOa2b9/ueeaOO+7wPFNaWup55s9//rPnGcASV8MGACQlAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEFyMFzjNu3DjPM++9957nmba2Ns8zb7/9tueZffv2eZ6RpNWrV3ueSbIvJUgCXIwUAJCUCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATXIwUuEzz58/3PLN27VrPM+np6Z5nYvXkk096nlm/fr3nmebmZs8zGDi4GCkAICkRIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACa4GClgYNKkSZ5nXnrpJc8zs2bN8jwTqzVr1nieee655zzP/Pvf//Y8AxtcjBQAkJQIEADAhOcA7d69W3PnzlV+fr58Pp+2bt0adf8DDzwgn88XtebMmROv/QIAUoTnAHV0dGjKlClavXp1n8fMmTNHzc3NkbVx48bL2iQAIPUM8TpQWlqq0tLSix7j9/sVDAZj3hQAIPUl5GdA1dXVysnJ0YQJE/TII4/oxIkTfR7b1dWlcDgctQAAqS/uAZozZ47Wr1+vqqoq/fSnP1VNTY1KS0vV3d3d6/GVlZUKBAKRNWrUqHhvCQCQhDx/C+5S7r333sifb775Zk2ePFnjxo1TdXV1r38noaKiQsuWLYt8HA6HiRAAXAES/jbssWPHKjs7W/X19b3e7/f7lZGREbUAAKkv4QE6evSoTpw4oby8vEQ/FABgAPH8LbiTJ09GvZppaGjQwYMHlZWVpaysLD377LNauHChgsGgjhw5oieeeELjx49XSUlJXDcOABjYPAdo3759uvPOOyMff/zzm0WLFumVV17RoUOH9Jvf/EZtbW3Kz8/X7Nmz9aMf/Uh+vz9+uwYADHhcjBQYIDIzMz3PzJ07N6bHWrt2recZn8/neWbXrl2eZ+666y7PM7DBxUgBAEmJAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJrgaNoALdHV1eZ4ZMsTzb3fRRx995Hkmlt8tVl1d7XkGl4+rYQMAkhIBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYML71QMBXLbJkyd7nvn617/ueWbatGmeZ6TYLiwaiw8++MDzzO7duxOwE1jgFRAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIKLkQLnmTBhgueZ8vJyzzMLFizwPBMMBj3P9Kfu7m7PM83NzZ5nenp6PM8gOfEKCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwwcVIkfRiuQjnfffdF9NjxXJh0euvvz6mx0pm+/bt8zzz3HPPeZ75wx/+4HkGqYNXQAAAEwQIAGDCU4AqKys1bdo0paenKycnR/PmzVNdXV3UMZ2dnSorK9OIESN0zTXXaOHChWptbY3rpgEAA5+nANXU1KisrEx79uzRm2++qbNnz2r27Nnq6OiIHLN06VJt375dmzdvVk1NjY4dOxbTL98CAKQ2T29C2LlzZ9TH69atU05Ojvbv368ZM2YoFArp17/+tTZs2KAvf/nLkqS1a9fqs5/9rPbs2aMvfvGL8ds5AGBAu6yfAYVCIUlSVlaWJGn//v06e/asiouLI8dMnDhRo0ePVm1tba+fo6urS+FwOGoBAFJfzAHq6enRkiVLdOutt2rSpEmSpJaWFqWlpSkzMzPq2NzcXLW0tPT6eSorKxUIBCJr1KhRsW4JADCAxBygsrIyvf/++9q0adNlbaCiokKhUCiympqaLuvzAQAGhpj+Imp5ebl27Nih3bt3a+TIkZHbg8Ggzpw5o7a2tqhXQa2trX3+ZUK/3y+/3x/LNgAAA5inV0DOOZWXl2vLli3atWuXCgoKou6fOnWqhg4dqqqqqshtdXV1amxsVFFRUXx2DABICZ5eAZWVlWnDhg3atm2b0tPTIz/XCQQCGjZsmAKBgB588EEtW7ZMWVlZysjI0KOPPqqioiLeAQcAiOIpQK+88ookaebMmVG3r127Vg888IAk6ec//7kGDRqkhQsXqqurSyUlJfrVr34Vl80CAFKHzznnrDdxvnA4rEAgYL0NfAq5ubmeZz73uc95nvnlL3/peWbixImeZ5Ld3r17Pc+88MILMT3Wtm3bPM/09PTE9FhIXaFQSBkZGX3ez7XgAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYCKm34iK5JWVleV5Zs2aNTE91i233OJ5ZuzYsTE9VjJ75513PM+8+OKLnmf+9Kc/eZ45ffq05xmgv/AKCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwwcVI+0lhYaHnmeXLl3uemT59uueZ6667zvNMsjt16lRMc6tWrfI885Of/MTzTEdHh+cZINXwCggAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMMHFSPvJ/Pnz+2WmP33wwQeeZ3bs2OF55qOPPvI88+KLL3qekaS2traY5gB4xysgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMCEzznnrDdxvnA4rEAgYL0NAMBlCoVCysjI6PN+XgEBAEwQIACACU8Bqqys1LRp05Senq6cnBzNmzdPdXV1UcfMnDlTPp8vai1evDiumwYADHyeAlRTU6OysjLt2bNHb775ps6ePavZs2ero6Mj6riHHnpIzc3NkbVy5cq4bhoAMPB5+o2oO3fujPp43bp1ysnJ0f79+zVjxozI7VdffbWCwWB8dggASEmX9TOgUCgkScrKyoq6/bXXXlN2drYmTZqkiooKnTp1qs/P0dXVpXA4HLUAAFcAF6Pu7m731a9+1d16661Rt69Zs8bt3LnTHTp0yP3ud79z1113nZs/f36fn2fFihVOEovFYrFSbIVCoYt2JOYALV682I0ZM8Y1NTVd9LiqqionydXX1/d6f2dnpwuFQpHV1NRkftJYLBaLdfnrUgHy9DOgj5WXl2vHjh3avXu3Ro4cedFjCwsLJUn19fUaN27cBff7/X75/f5YtgEAGMA8Bcg5p0cffVRbtmxRdXW1CgoKLjlz8OBBSVJeXl5MGwQApCZPASorK9OGDRu0bds2paenq6WlRZIUCAQ0bNgwHTlyRBs2bNBXvvIVjRgxQocOHdLSpUs1Y8YMTZ48OSH/AACAAcrLz33Ux/f51q5d65xzrrGx0c2YMcNlZWU5v9/vxo8f75YvX37J7wOeLxQKmX/fksVisViXvy71tZ+LkQIAEoKLkQIAkhIBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwETSBcg5Z70FAEAcXOrredIFqL293XoLAIA4uNTXc59LspccPT09OnbsmNLT0+Xz+aLuC4fDGjVqlJqampSRkWG0Q3uch3M4D+dwHs7hPJyTDOfBOaf29nbl5+dr0KC+X+cM6cc9fSqDBg3SyJEjL3pMRkbGFf0E+xjn4RzOwzmch3M4D+dYn4dAIHDJY5LuW3AAgCsDAQIAmBhQAfL7/VqxYoX8fr/1VkxxHs7hPJzDeTiH83DOQDoPSfcmBADAlWFAvQICAKQOAgQAMEGAAAAmCBAAwMSACdDq1at1/fXX66qrrlJhYaHeffdd6y31u2eeeUY+ny9qTZw40XpbCbd7927NnTtX+fn58vl82rp1a9T9zjk9/fTTysvL07Bhw1RcXKzDhw/bbDaBLnUeHnjggQueH3PmzLHZbIJUVlZq2rRpSk9PV05OjubNm6e6urqoYzo7O1VWVqYRI0bommuu0cKFC9Xa2mq048T4NOdh5syZFzwfFi9ebLTj3g2IAL3++utatmyZVqxYoffee09TpkxRSUmJjh8/br21fnfTTTepubk5sv7yl79YbynhOjo6NGXKFK1evbrX+1euXKlVq1bp1Vdf1d69ezV8+HCVlJSos7Ozn3eaWJc6D5I0Z86cqOfHxo0b+3GHiVdTU6OysjLt2bNHb775ps6ePavZs2ero6MjcszSpUu1fft2bd68WTU1NTp27JgWLFhguOv4+zTnQZIeeuihqOfDypUrjXbcBzcATJ8+3ZWVlUU+7u7udvn5+a6ystJwV/1vxYoVbsqUKdbbMCXJbdmyJfJxT0+PCwaD7oUXXojc1tbW5vx+v9u4caPBDvvHJ8+Dc84tWrTI3X333Sb7sXL8+HEnydXU1Djnzv27Hzp0qNu8eXPkmL///e9OkqutrbXaZsJ98jw459wdd9zhHnvsMbtNfQpJ/wrozJkz2r9/v4qLiyO3DRo0SMXFxaqtrTXcmY3Dhw8rPz9fY8eO1f3336/GxkbrLZlqaGhQS0tL1PMjEAiosLDwinx+VFdXKycnRxMmTNAjjzyiEydOWG8poUKhkCQpKytLkrR//36dPXs26vkwceJEjR49OqWfD588Dx977bXXlJ2drUmTJqmiokKnTp2y2F6fku5ipJ/04Ycfqru7W7m5uVG35+bm6h//+IfRrmwUFhZq3bp1mjBhgpqbm/Xss8/q9ttv1/vvv6/09HTr7ZloaWmRpF6fHx/fd6WYM2eOFixYoIKCAh05ckRPPvmkSktLVVtbq8GDB1tvL+56enq0ZMkS3XrrrZo0aZKkc8+HtLQ0ZWZmRh2bys+H3s6DJH3zm9/UmDFjlJ+fr0OHDun73/++6urq9Pvf/95wt9GSPkD4f6WlpZE/T548WYWFhRozZozeeOMNPfjgg4Y7QzK49957I3+++eabNXnyZI0bN07V1dWaNWuW4c4So6ysTO+///4V8XPQi+nrPDz88MORP998883Ky8vTrFmzdOTIEY0bN66/t9mrpP8WXHZ2tgYPHnzBu1haW1sVDAaNdpUcMjMzdeONN6q+vt56K2Y+fg7w/LjQ2LFjlZ2dnZLPj/Lycu3YsUNvv/121K9vCQaDOnPmjNra2qKOT9XnQ1/noTeFhYWSlFTPh6QPUFpamqZOnaqqqqrIbT09PaqqqlJRUZHhzuydPHlSR44cUV5envVWzBQUFCgYDEY9P8LhsPbu3XvFPz+OHj2qEydOpNTzwzmn8vJybdmyRbt27VJBQUHU/VOnTtXQoUOjng91dXVqbGxMqefDpc5Dbw4ePChJyfV8sH4XxKexadMm5/f73bp169wHH3zgHn74YZeZmelaWlqst9avvve977nq6mrX0NDg/vrXv7ri4mKXnZ3tjh8/br21hGpvb3cHDhxwBw4ccJLcSy+95A4cOOD+9a9/Oeece/75511mZqbbtm2bO3TokLv77rtdQUGBO336tPHO4+ti56G9vd09/vjjrra21jU0NLi33nrLff7zn3c33HCD6+zstN563DzyyCMuEAi46upq19zcHFmnTp2KHLN48WI3evRot2vXLrdv3z5XVFTkioqKDHcdf5c6D/X19e6HP/yh27dvn2toaHDbtm1zY8eOdTNmzDDeebQBESDnnPvFL37hRo8e7dLS0tz06dPdnj17rLfU7+655x6Xl5fn0tLS3HXXXefuueceV19fb72thHv77bedpAvWokWLnHPn3or91FNPudzcXOf3+92sWbNcXV2d7aYT4GLn4dSpU2727Nnu2muvdUOHDnVjxoxxDz30UMr9T1pv//yS3Nq1ayPHnD592n33u991n/nMZ9zVV1/t5s+f75qbm+02nQCXOg+NjY1uxowZLisry/n9fjd+/Hi3fPlyFwqFbDf+Cfw6BgCAiaT/GRAAIDURIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACb+Dwuo74MxItlsAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a900640-93a5-4d45-8c35-89b541a0e1f3",
   "metadata": {},
   "source": [
    "## Creando nuestra red neuronal\n",
    "\n",
    "Vimos que una red neuronal es una composición de funciones que llevan de $\\large \\mathbb{R}^{n_{in}}$ a $\\large \\mathbb{R}^{n_{out}}$, como composición de funciones que transforman vectores en vectores:\n",
    "\n",
    "$\\Large \\mathbb{R}^{n_{in}} \\rightarrow \\mathbb{R}^{n_{hidden_1}} \\rightarrow \\mathbb{R}^{n_{hidden_2}} \\rightarrow ... \\rightarrow \\mathbb{R}^{n_{out}}$\n",
    "\n",
    "Vamos a entrenar una red de dos capas ocultas de tamaño 512. O sea concretamente nuestra red tendrá la forma:\n",
    "\n",
    "$\\Large \\mathbb{R}^{784} \\rightarrow \\mathbb{R}^{512} \\rightarrow \\mathbb{R}^{512} \\rightarrow \\mathbb{R}^{10}$\n",
    "\n",
    "Como vimos cada función toma los valores de la capa anterior $h_{t}$ y produce los de la capa subsiguiente $h_{t+1}$ aplicando una transformación lineal seguida de una función no-lineal $g$:\n",
    "\n",
    "$h_{t+1} = g( h_t \\times W_t + b_t )$\n",
    "\n",
    "$g$ será la función $ReLU$ excepto para la última capa que utilizaremos la función softmax:\n",
    "\n",
    "$ReLU(x) = max(0, x)$\n",
    "\n",
    "La función softmax toma un vector de longitud $n$, lo positiviza aplicando $e^x$ y luego divide cada componente por la suma de cada uno de dichos términos positivos. Como resultado da un vector de longitud $n$ que es siempre positivo y la suma es $1$. Se utiliza cuando se espera que la salida sea una distribución de probabilidad. En problemas de clasificación, la salida son $n$ probabilidades siendo $n$ el número de clases (la probabilidad que da el modelo para que el $x$ suministrado pertenezca a cada una de las clases).\n",
    "\n",
    "Los pesos de cada capa son inicializados de manera aleatoria con una fórmula que los escala teniendo en cuenta la dimensión de entrada y salida. Lo que se pretende con esto es que los valores de las activaciones no crezcan demasiado, ya que de hacerlo luego e^x (aplicado en la última capa por la softmax resultará en el valor inf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e6becad6-5741-4fe2-9333-545827bac5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0268, 0.0524, 0.0326, 0.0822, 0.0066, 0.0279, 0.6542, 0.0038, 0.1013,\n",
       "         0.0121], grad_fn=<DivBackward0>),\n",
       " torch.Size([10]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def w_rand(n_in, n_out):\n",
    "    return torch.normal(0, math.sqrt(6) / math.sqrt(n_in + n_out), size=(n_in,n_out), requires_grad=True)\n",
    "\n",
    "W_1 = w_rand(28*28, 512)\n",
    "b_1 = torch.zeros(512, requires_grad=True)\n",
    "\n",
    "W_2 = w_rand(512, 512)\n",
    "b_2 = torch.zeros(512, requires_grad=True)\n",
    "\n",
    "W_3 = w_rand(512,10)\n",
    "b_3 = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "weights = [W_1, b_1, W_2, b_2, W_2, b_3]\n",
    "\n",
    "def softmax(x):\n",
    "    return x.exp() / x.exp().sum(-1).unsqueeze(-1)\n",
    "\n",
    "def calculate_predictions(t):\n",
    "    linear_combination_1 = t @ W_1 + b_1\n",
    "    hidden_layer_1 = torch.max(torch.tensor(0), linear_combination_1)\n",
    "    \n",
    "    linear_combination_2 = hidden_layer_1 @ W_2 + b_2\n",
    "    hidden_layer_2 = torch.max(torch.tensor(0), linear_combination_2)\n",
    "\n",
    "    linear_combination_3 = hidden_layer_2 @ W_3 + b_3\n",
    "    \n",
    "    return softmax(linear_combination_3)\n",
    "\n",
    "pred = calculate_predictions(torch.tensor(x_train[0]))\n",
    "pred, pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc2dd9b-1194-4f6d-bdae-b4e2275608e1",
   "metadata": {},
   "source": [
    "Podemos corroborar que efectivamente la salida suma 1 (como debe ser por ser una distribución de probabilidad):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84ba3545-13ea-4d0a-abc3-8b7d95293729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847e2b47-04fe-469b-9598-6dbff087e416",
   "metadata": {},
   "source": [
    "Necesitamos una función que nos genere los mini-batches, no queremos evaluar los 50000 arrays en cada iteración de descenso por gradiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3a40f52-9d03-4bfb-84d1-befd6db429de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 784]), torch.Size([32]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "def get_batch(a, i):\n",
    "    return torch.tensor(a[batch_size*i:batch_size*(i+1),...])\n",
    "\n",
    "x_batch = get_batch(x_train, 0)\n",
    "y_batch = get_batch(y_train, 0)\n",
    "\n",
    "x_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828b7f38-d97d-45d8-82dd-7bece4f69d70",
   "metadata": {},
   "source": [
    "La misma función que calculate_predictions, que nos servía para evaluar una sóla instancia la podemos utilizar para calcular la predicción de un mini-batch entero. Esto es posible gracias al broadcasting.\n",
    "\n",
    "Se calcula de forma paralela, lo cual es ventajoso si estamos usando GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c21ab6eb-3042-4330-9de0-a222a35cf06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_pred = calculate_predictions(x_batch)\n",
    "batch_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841e8996-96ec-4c1d-b461-e173499929b9",
   "metadata": {},
   "source": [
    "Como función de costo utilizaremos Negative Log Likehood (NNL). O sea el $log$ de la probabilidad de la clase objetivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b9bcf74-f90b-4c05-b679-7368296328c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.1815, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def negative_log_likehood(pred_batch, target_batch):\n",
    "    bz = pred_batch.shape[0]\n",
    "    return -torch.log(pred_batch[range(bz), target_batch]).mean()\n",
    "\n",
    "negative_log_likehood(batch_pred, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f214ba18-923b-4100-9866-d704c20a8534",
   "metadata": {},
   "source": [
    "La función anterior asume que los argumentos son mini-batches de predicciones y clases (no va a funcionar dado sólo una predicción):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3171c7f3-005f-4c9e-aa3c-cbf4d0cad0fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnegative_log_likehood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m, in \u001b[0;36mnegative_log_likehood\u001b[0;34m(pred_batch, target_batch)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnegative_log_likehood\u001b[39m(pred_batch, target_batch):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlog(\u001b[43mpred_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "negative_log_likehood(pred, y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36fedb0-dfcc-4fc8-80a2-15c39313ace5",
   "metadata": {},
   "source": [
    "Lo que podemos hacer en estos casos si queremos evaluarlo en una instancia sola es crear un mini-batch de tamaño 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "004995c5-82a6-4eec-a3bf-c01ab4e7fad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 784]), torch.Size([1, 1]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(x_train[0])\n",
    "y = torch.tensor([y_train[0]])\n",
    "x = x.unsqueeze(0)\n",
    "y = y.unsqueeze(0)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2295bdc3-867e-43ec-a49d-139598340819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2993, 0.0299, 0.1423, 0.0729, 0.0143, 0.0448, 0.0307, 0.1609, 0.0131,\n",
       "          0.1918]], grad_fn=<DivBackward0>),\n",
       " torch.Size([1, 10]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = calculate_predictions(x)\n",
    "pred, pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "90e99409-3d86-4b21-a3e3-5711f5355aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.1063, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_log_likehood(pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a47767-498e-4371-a19f-6c2bb6feb257",
   "metadata": {},
   "source": [
    "Como métrica de evaluación utilizaremos la accuracy (tasa de aciertos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5ac17d4d-a1f3-497b-b419-f95c9ae2fd9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1875)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(probs, target):\n",
    "    class_predictions = torch.argmax(probs, dim=1)\n",
    "    return (class_predictions == target).float().mean()\n",
    "\n",
    "accuracy(batch_pred, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c09fda6-9bf2-47e4-a865-fc2fb2d25ef0",
   "metadata": {},
   "source": [
    "Ya estamos en condición de escribir nuestro loop de entrenamiento:\n",
    "- Para cada mini-batch del train haremos un paso de descenso por gradiente:\n",
    "    - Evaluaremos el modelo en nuestro mini-batch.\n",
    "    - Calcularemos el costo: negative log likehood.\n",
    "    - Calcularemos los gradientes del costo respecto de los pesos.\n",
    "    - Actualizaremos los gradientes de los pesos. \n",
    "- Esto lo repetiremos una cierta cantidad de épocas.\n",
    "- Al final de cada época evaluaremos el dataset de validación y mostraremos el valor de la métrica (accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "51fb7b90-591d-4cf7-88ab-a9e3b9860db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 0.5052425868737812 | validation loss 0.9777122660040397 | accuracy 0.8692092651757188\n",
      "epoch 1 | train loss 0.3738938947431386 | validation loss 0.4698387107569796 | accuracy 0.9009584664536742\n",
      "epoch 2 | train loss 0.3231457392342936 | validation loss 0.3804546149874466 | accuracy 0.9135383386581469\n",
      "epoch 3 | train loss 0.2943848981834448 | validation loss 0.33608312579527705 | accuracy 0.9211261980830671\n",
      "epoch 4 | train loss 0.2745885700642015 | validation loss 0.30787961049990376 | accuracy 0.9243210862619808\n",
      "epoch 5 | train loss 0.2594449375027094 | validation loss 0.2874436013963403 | accuracy 0.9286142172523961\n",
      "epoch 6 | train loss 0.24811098111656527 | validation loss 0.2711057163221659 | accuracy 0.9326078274760383\n",
      "epoch 7 | train loss 0.23852265776155857 | validation loss 0.25805524648638223 | accuracy 0.9339057507987221\n",
      "epoch 8 | train loss 0.22940053103473812 | validation loss 0.2467383164507021 | accuracy 0.9374001597444089\n",
      "epoch 9 | train loss 0.22159676642689746 | validation loss 0.2369379970649312 | accuracy 0.939297124600639\n",
      "epoch 10 | train loss 0.2158487515506177 | validation loss 0.22810630656292283 | accuracy 0.9418929712460063\n",
      "epoch 11 | train loss 0.20902028631347533 | validation loss 0.22054414092068217 | accuracy 0.9434904153354633\n",
      "epoch 12 | train loss 0.2036022837377918 | validation loss 0.213291603582941 | accuracy 0.9454872204472844\n",
      "epoch 13 | train loss 0.1999552194088602 | validation loss 0.20678291134488566 | accuracy 0.9453873801916933\n",
      "epoch 14 | train loss 0.19472314207293925 | validation loss 0.20104322706142896 | accuracy 0.9471845047923323\n"
     ]
    }
   ],
   "source": [
    "n_samples_train = x_train.shape[0]\n",
    "n_samples_valid = x_valid.shape[0]\n",
    "\n",
    "n_batches_train = n_samples_train // batch_size\n",
    "n_batches_valid = (n_samples_valid + batch_size - 1) // batch_size\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_epochs = 15\n",
    "\n",
    "for idx_epoch in range(n_epochs):\n",
    "    # Loop de entrenamiento\n",
    "    loss_train_sum = 0\n",
    "    for idx_batch in range(n_batches_train):\n",
    "        # Obtenemos un mini-batch de entrenamiento\n",
    "        x_train_batch, y_train_batch = get_batch(x_train, idx_batch), get_batch(y_train, idx_batch)\n",
    "        \n",
    "        # Calculamos las predciones del modelo para dicho mini-batch\n",
    "        predictions = calculate_predictions(x_train_batch)\n",
    "    \n",
    "        # Calculamos el costo de dichas predicciones\n",
    "        loss = negative_log_likehood(predictions, y_train_batch)\n",
    "    \n",
    "        # Calculamos el gradiente de los pesos\n",
    "        # Antes de llamar a backwards, inicializarmos los gradientes en cero. \n",
    "        # Recordar que Pytorch acumula los gradientes, de modo que si llamamos \n",
    "        # a backwards dos veces sin inicializar en cero, se acumularán ambos cálculos.\n",
    "        for w in weights:\n",
    "            if w.grad is not None:\n",
    "                w.grad.zero_()\n",
    "        loss.backward()\n",
    "        loss_train_sum += loss.item()\n",
    "    \n",
    "        # Actualizaremos los pesos utilizando los gradientes, según la fórmula de descenso por gradiente\n",
    "        for w in weights:\n",
    "            w.data -= w.grad.data * learning_rate\n",
    "        \n",
    "    # Una vez terminado el entrenamiento\n",
    "    # Shuffleamos los datos de entrenamiento de modo que los mini-batches de la próxima epoch sean distintos\n",
    "    idxs_rand = torch.randperm(x_train.shape[0])\n",
    "    x_train = x_train[idxs_rand,...]\n",
    "    y_train = y_train[idxs_rand,...]\n",
    "\n",
    "    # Evaluamos los datos en validación\n",
    "    loss_validation_sum = 0\n",
    "    accuracy_sum = 0\n",
    "    for idx_batch in range(n_batches_valid):\n",
    "        # Obtenemos un mini-batch de validación\n",
    "        x_valid_batch, y_valid_batch = get_batch(x_valid, idx_batch), get_batch(y_valid, idx_batch)\n",
    "        predictions = calculate_predictions(x_valid_batch)\n",
    "        loss = negative_log_likehood(predictions, y_valid_batch)\n",
    "        loss_validation_sum += loss.item()\n",
    "\n",
    "        # Evaluamos nuestra métrica. NUNCA lo hacemos en train\n",
    "        accuracy_sum += accuracy(predictions, y_valid_batch).item()\n",
    "    \n",
    "    # Imprimimos el loss en train y validación y la métrica (siempre en validación)\n",
    "    accuracy_validation = accuracy_sum / n_batches_valid\n",
    "    loss_validation = loss_validation_sum / n_batches_valid\n",
    "    train_validation = loss_train_sum / n_batches_train\n",
    "    print(f'epoch {idx_epoch} | train loss {loss_validation} | validation loss {train_validation} | accuracy {accuracy_validation}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482794a2-732b-42cd-a448-cc02509db6dc",
   "metadata": {},
   "source": [
    "Una accuracy del 94% es bastante considerando que tenemos 10 clases.\n",
    "\n",
    "Luego, veamos como usar nuestro modelo si tenemos un imagen en tiempo de inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "baba75d0-a2e8-4864-ae48-7b3625bc21fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x_valid[15]\n",
    "y = y_valid[15]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f22bce83-b68d-4d33-ac90-79d5d771d169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.imshow(x.reshape((28, 28)), cmap=\"gray\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a464f4ce-7361-4850-b62b-fe21e2f2a412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1632e-05, 7.8863e-06, 1.4811e-04, 3.7708e-03, 4.1223e-05, 1.6870e-03,\n",
       "        1.5630e-06, 1.4821e-06, 9.9334e-01, 9.8649e-04],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = calculate_predictions(torch.tensor(x))\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c049ed2-54ef-47c6-890a-9cf209a4e626",
   "metadata": {},
   "source": [
    "El modelo le acertó esta vez, la probabildiad más alta es de que sea un 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a0ec910e-bb69-48f9-b15e-7aa238909a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6a0054-7cb0-40f0-addf-a2f25be94379",
   "metadata": {},
   "source": [
    "#### Ejercicio Opcional:\n",
    "\n",
    "Copiar esta notebook y borrar las celdas donde creamos la red neuronal (desde la sección \"Creando nuestra red neuronal\" en adelante). De modo que sólo quede la parte donde importamos el dataset. Y re-escribir:\n",
    "- La creación de los pesos y la función que dado las features produce la predicción.\n",
    "- La función de costo (negative log likehood).\n",
    "- La métrica: accuracy.\n",
    "- El loop de entrenamiento, incluyendo código para evaluar en cada época y mostrar cómo mejora la métrica.\n",
    "- Un ejemplo de cómo se usa el modelo para inferencia (sobre un sólo caso, para calcular la predicción)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46686215-a53e-45b3-9d20-03a523fb02e7",
   "metadata": {},
   "source": [
    "## Utilizando Pytorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7117c1-0d9c-4c93-801e-26f4cfd31260",
   "metadata": {},
   "source": [
    "Pytorch define clases y funciones que proveen las abstracciones comunes a trabajar con redes neuronales.\n",
    "\n",
    "Una abstracción es la de Dataset. Un Daataset es una lista de ejemplos, sobre el que se va a entrenar. Existen diferentes tipos de Dataset, TensorDataset es un tipo heredado de Dataset, que sirve para definir una lista dejemplo dado tensores de features y samples.\n",
    "\n",
    "Uno puede crear sus propios tipos de datasets tal como se muetra en este [tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
    "\n",
    "Para nuestro fin, TensorDataset nos viene bien, ya que tenemos los samples en tensores (tanto las features como las targets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "83c99630-547a-4b86-a141-faaeefa35ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(x_train), torch.tensor(y_train))\n",
    "valid_dataset = TensorDataset(torch.tensor(x_valid), torch.tensor(y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de45056-3bf0-44b1-9f74-5e43821119f9",
   "metadata": {},
   "source": [
    "Un dataset se puede indexar y obtener un ejemplo que será una tupla de (features, target):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a81522bb-7dfa-48c8-ac1f-7886dbd13663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, 2)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset[0]), len(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7b69fd54-7067-4fac-869f-0e48fbe44fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), tensor(7))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].shape, train_dataset[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9053d118-6680-44c1-8b67-9368ac873a79",
   "metadata": {},
   "source": [
    "Otra abstracción que nos provee pytorch es el de DataLoader, que nos define un generador de batches dado un Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "723b4531-1a51-4c3a-bd5d-332b096def5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86203026-1443-4fc6-9b23-38c4d0d73c6d",
   "metadata": {},
   "source": [
    "Los DataLoaders no son indexables: no podemos obtener el batch i-ésimo. Sino que son generadores: permite iterar sobre los batches.\n",
    "\n",
    "Además la clase DataLoader provee funcionalidad aparejada al concepto de mini-batch: por ejemplo shufflear (randomizar) el orden de los ejemplos una vez que se itere sobre todo el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ad3473a1-182f-49da-ad9a-ab8d5194ec59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 784]), torch.Size([32]))"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch, y_batch = next(iter(train_dataloader))\n",
    "x_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07711148-c753-4c0a-9b9c-ff0d5a8463bb",
   "metadata": {},
   "source": [
    "Una de las abstracciones más poderosas que posee Pytorch es la de nn.Module, para definir modelos. Un modelo escencialmente es una función que contiene parámetros. \n",
    "\n",
    "Los modelos pueden estar compuestos de modelos más simples. Por ejemplo nuestro modelo contiene tres variables de tipo nn.Linear (la clase nn.Linear hereda de nn.Module).\n",
    "\n",
    "Heredando de nn.Module estamos creando nuestro propia arquitectura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a625f350-f768-4dcc-9095-09e628523a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MyArchitecture(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_layer_1 = nn.Linear(28*28, 512)\n",
    "        self.linear_layer_2 = nn.Linear(512, 512)\n",
    "        self.linear_layer_3 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        l1 = self.linear_layer_1(x)\n",
    "        h1 = nn.ReLU()(l1)\n",
    "        l2 = self.linear_layer_2(h1)\n",
    "        h2 = nn.ReLU()(l2)\n",
    "        l3 = self.linear_layer_3(h2)\n",
    "        probs = nn.LogSoftmax(dim=-1)(l3)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6c68a4-8e2e-4c69-851c-33a71718dc8a",
   "metadata": {},
   "source": [
    "Luego podemos crear un modelo instanciando la clase definida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "70c979a3-e69c-4718-ae3f-78e14fff9720",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyArchitecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2e20c3-9e59-4925-8760-a7880ef2ee29",
   "metadata": {},
   "source": [
    "Cuando llamemos al modelo en una entrada determinada, se ejecutará el código de la función forward para dicha entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "68200b07-9a10-48f6-bfbd-6d75c1b44ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-2.3114, -2.2874, -2.3168, -2.3591, -2.3102, -2.3324, -2.2724, -2.2291,\n",
       "         -2.3245, -2.2883], grad_fn=<LogSoftmaxBackward0>),\n",
       " torch.Size([10]))"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs = model(train_dataset[0][0])\n",
    "log_probs, log_probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3304a8e0-44fc-461b-aa27-b40439be0a74",
   "metadata": {},
   "source": [
    "Notar que el modelo es ligeramente distinto al implementado anteriormente: las predicciones no son probabilidades sino el log de las probabilidades. Esto es debido a que calcular el log del softmax es fácil debido a un trabajo algebraico (el softmax es un cociente y el log del cociente es la diferencia de logs). Y de todas formas le íbamos a aplicar log para la función de costo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "fd628d15-6969-418f-8048-da83982e5f2d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0991, 0.1015, 0.0986, 0.0945, 0.0992, 0.0971, 0.1031, 0.1076, 0.0978,\n",
       "        0.1014], grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "1f61f925-c76c-4fd1-974f-eff1d35e39d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(log_probs).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205f4b2b-026a-4073-a86c-ffede12f3172",
   "metadata": {},
   "source": [
    "Gracias al broadcasting, podemos llamar al modelo en un mini-batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "c26e6c72-deec-4c6c-8d94-005deafbd83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs_batch = model(x_batch)\n",
    "log_probs_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f037151-2ce9-4fb1-92ef-92e4a8d127b5",
   "metadata": {},
   "source": [
    "nn.Module implementa muchas funcionalidades comunes a los modelos como por ejemplo mostrar las variables que lo componen (notar que no muestra como se combinan dichas variables):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "6bf0b36a-2826-4c91-9e1f-131f9fab8b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyArchitecture(\n",
       "  (linear_layer_1): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (linear_layer_2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (linear_layer_3): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f062b3-8e71-4f61-8c50-dcc5957599e9",
   "metadata": {},
   "source": [
    "O también listas los parámetros (muy útil para actualizarlos, nosotros en nuestra implementación anterior tuvimos que crear manualmente una lista a la cual llamamos weights de todos los parámetros a actualizar):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "76684a7f-4387-451f-981d-25d9e5ed2370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0039,  0.0006,  0.0080,  ...,  0.0202,  0.0259, -0.0136],\n",
       "         [-0.0223, -0.0252, -0.0076,  ...,  0.0113,  0.0170,  0.0354],\n",
       "         [-0.0064, -0.0170,  0.0080,  ..., -0.0148,  0.0064, -0.0043],\n",
       "         ...,\n",
       "         [ 0.0018,  0.0209,  0.0322,  ..., -0.0317,  0.0072, -0.0111],\n",
       "         [ 0.0228,  0.0205, -0.0291,  ..., -0.0048,  0.0223, -0.0288],\n",
       "         [-0.0158,  0.0215,  0.0341,  ...,  0.0225, -0.0036,  0.0082]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-2.5272e-03, -2.4023e-02,  6.0572e-03, -3.4850e-02, -1.5220e-03,\n",
       "          4.3627e-03,  1.6719e-02,  3.2558e-02,  2.9371e-02,  1.4942e-02,\n",
       "         -2.0186e-02,  1.7683e-02, -2.5695e-03, -3.5563e-02,  2.3919e-02,\n",
       "          9.0389e-03, -3.0987e-02, -1.1462e-02, -1.0536e-02,  1.2851e-02,\n",
       "         -3.2080e-02,  1.0557e-02, -2.5109e-02, -2.3409e-02, -2.9000e-02,\n",
       "          2.7170e-03,  2.2378e-02,  6.2972e-04, -1.4320e-02, -1.9974e-02,\n",
       "          1.0752e-02, -1.4607e-02,  1.1390e-02, -2.3970e-02,  1.4762e-02,\n",
       "          1.2173e-02,  3.1721e-02,  1.8361e-03, -1.8112e-02, -7.8636e-03,\n",
       "         -6.3021e-03,  3.9408e-03,  2.3681e-02,  2.6562e-02, -1.9471e-02,\n",
       "         -1.8510e-02,  4.2479e-03,  3.2281e-02, -7.9352e-03,  7.3923e-03,\n",
       "         -6.1848e-03, -2.7890e-02, -5.9744e-03, -1.6964e-02, -3.0161e-02,\n",
       "          3.2845e-02,  1.9707e-02,  1.7198e-02, -5.5234e-03, -2.8286e-02,\n",
       "         -2.5061e-02,  1.9338e-02,  1.5804e-02, -2.8030e-02, -2.2059e-02,\n",
       "          2.7980e-03, -2.5095e-02,  3.1436e-02, -6.4170e-03, -3.2011e-02,\n",
       "          2.1755e-02,  1.7678e-02, -2.6253e-03,  1.3648e-03,  2.5617e-02,\n",
       "          2.3947e-02,  7.9349e-03,  1.0134e-02, -1.1915e-02,  2.2332e-02,\n",
       "          8.6957e-03, -3.4989e-02,  1.7125e-02, -1.1083e-02, -4.1395e-03,\n",
       "         -2.3539e-02, -3.1305e-03,  2.3727e-02, -3.1690e-02, -4.4242e-03,\n",
       "         -1.1119e-03, -9.2740e-03, -2.6770e-02,  4.9288e-03,  5.6286e-03,\n",
       "         -1.4252e-02,  2.6567e-02, -3.3459e-02,  2.3794e-02,  3.1671e-02,\n",
       "         -1.6787e-02,  4.3471e-03,  3.4628e-02,  3.1763e-02, -3.0628e-02,\n",
       "         -6.8659e-03, -3.8718e-03, -2.7573e-02, -2.4691e-02, -6.4001e-03,\n",
       "          3.2915e-02, -1.0875e-02, -3.6714e-03, -2.3196e-02, -2.7644e-02,\n",
       "         -4.5250e-03,  6.1943e-03, -3.3242e-02,  8.6836e-03, -3.5101e-02,\n",
       "          3.5391e-02,  2.5416e-02,  2.0524e-02, -2.5287e-03, -3.1632e-02,\n",
       "         -4.5451e-03,  1.8688e-02,  3.4890e-02,  2.9652e-02, -2.4520e-02,\n",
       "          1.4951e-02, -1.5055e-02,  3.0025e-02, -2.2793e-02, -9.7898e-03,\n",
       "          3.0803e-02,  2.6523e-02,  2.3790e-02, -2.6766e-02, -3.4822e-02,\n",
       "         -4.8642e-03,  3.4804e-02,  3.0739e-02,  4.3110e-03, -2.5602e-02,\n",
       "         -2.5730e-02, -1.5672e-02,  2.3058e-02, -2.5467e-02, -1.5536e-02,\n",
       "         -6.4625e-04, -9.6047e-03,  8.4994e-03,  2.0916e-03, -1.1519e-02,\n",
       "          1.3716e-02,  9.1829e-03, -2.7639e-02,  3.0733e-02, -2.2129e-02,\n",
       "         -5.9243e-03,  2.3444e-02, -1.2427e-02,  3.2344e-02,  5.7928e-03,\n",
       "          2.4381e-02, -2.2639e-02, -3.1567e-02,  2.6872e-02,  3.1448e-02,\n",
       "         -1.7758e-02, -2.4741e-02, -3.1299e-03,  2.8066e-02,  1.9932e-02,\n",
       "          3.9328e-03,  2.9426e-02, -3.4935e-02,  2.9110e-02, -2.6546e-02,\n",
       "          5.4559e-03,  1.6009e-02, -1.6373e-02,  1.4166e-02, -1.9865e-02,\n",
       "         -4.0738e-03,  2.1617e-02, -2.3923e-02, -3.4253e-02, -1.1034e-02,\n",
       "         -1.2451e-02,  3.5647e-02,  2.3163e-02,  2.3332e-02,  6.5693e-03,\n",
       "         -1.3282e-02, -2.7404e-02,  8.3309e-03,  1.0941e-02, -2.6197e-02,\n",
       "          2.2180e-02,  3.2374e-03,  2.1214e-02, -2.9399e-02,  3.5632e-02,\n",
       "          1.0476e-02, -1.3564e-02, -1.2118e-02,  1.1528e-02, -1.3020e-02,\n",
       "         -2.7271e-02, -2.6620e-02, -3.4166e-02,  1.7890e-02,  3.4631e-02,\n",
       "         -2.5024e-02, -9.1689e-03,  1.5258e-02, -3.4223e-02, -1.7141e-02,\n",
       "          3.5584e-02,  6.8669e-03, -5.9244e-03, -6.9059e-03,  1.0996e-02,\n",
       "         -2.9971e-02, -2.8108e-02, -1.6732e-02,  3.0101e-02,  1.8263e-02,\n",
       "          1.3494e-02, -2.5038e-02, -1.8682e-02,  7.1292e-03, -6.7461e-03,\n",
       "          2.8016e-02,  1.6331e-02, -3.3289e-02, -1.2651e-02,  6.5013e-03,\n",
       "          2.0498e-02, -2.6091e-02,  7.2320e-03,  1.6335e-02,  3.4272e-02,\n",
       "          8.6325e-03,  1.2671e-02, -3.3786e-02, -1.5867e-02,  1.5616e-02,\n",
       "          3.3737e-02,  2.9054e-02, -1.1771e-02, -1.0779e-02, -1.0363e-02,\n",
       "          1.1320e-02,  2.3310e-02, -4.8210e-03,  3.4083e-02, -2.7719e-02,\n",
       "         -3.1654e-02, -3.1904e-02,  2.0373e-02, -1.8691e-02,  1.7930e-03,\n",
       "         -1.7444e-02,  1.0798e-02,  1.2386e-02,  1.5446e-02, -1.9464e-02,\n",
       "          1.1200e-02,  3.6949e-03,  2.8543e-02, -2.1971e-02,  6.3715e-03,\n",
       "         -2.1453e-02, -1.8701e-02,  9.1059e-03, -1.7538e-02,  3.4726e-02,\n",
       "         -7.1163e-03,  4.6321e-03,  8.0707e-03,  2.7220e-02,  2.8867e-02,\n",
       "         -9.1898e-03,  2.3251e-02,  6.2797e-03, -2.5668e-02, -2.4939e-02,\n",
       "          3.1556e-02,  1.2337e-02, -4.5064e-03, -3.2005e-02, -2.7742e-02,\n",
       "          2.5886e-02,  2.6251e-02, -2.8263e-02,  3.1852e-02,  1.3880e-02,\n",
       "         -1.0279e-02,  3.3263e-02, -4.6159e-03, -1.9968e-02, -1.6957e-03,\n",
       "         -7.1011e-03, -6.6370e-03,  9.6822e-03,  1.9258e-02,  1.6024e-02,\n",
       "         -1.8192e-02, -1.2067e-02, -3.8320e-03,  6.4820e-03,  3.5433e-02,\n",
       "         -6.3336e-03, -2.0996e-02, -2.4476e-03, -2.5617e-02, -2.1599e-03,\n",
       "         -3.0902e-02,  1.0774e-02, -2.5338e-02, -3.9687e-03, -4.9130e-03,\n",
       "         -1.1515e-02,  3.5339e-02, -1.7321e-02, -1.8630e-02, -3.3510e-02,\n",
       "          1.4018e-02, -2.0065e-02,  1.3205e-03, -3.2471e-02, -8.7134e-03,\n",
       "         -1.0210e-02, -3.1389e-02, -4.4003e-03,  2.1464e-02, -3.2188e-02,\n",
       "          2.0753e-02, -1.4271e-02,  2.8861e-02,  1.8759e-02,  2.4181e-02,\n",
       "         -2.2640e-02,  5.0662e-03, -1.1030e-04, -2.0459e-02, -1.2046e-02,\n",
       "         -1.0906e-02, -1.3110e-02, -2.1587e-02,  3.3033e-03, -2.7677e-02,\n",
       "          4.7603e-03,  2.0254e-02, -1.5845e-02, -1.5062e-02, -2.1631e-02,\n",
       "          1.5808e-02, -1.9482e-02, -2.1504e-02,  1.0790e-02,  6.4356e-03,\n",
       "         -4.0731e-03, -2.3768e-02, -3.3718e-02, -2.5322e-02, -1.3916e-02,\n",
       "          2.0283e-02, -2.1411e-02, -2.0548e-02, -9.2968e-03,  2.6733e-02,\n",
       "          2.1017e-02, -1.9732e-02, -2.8710e-02,  2.6113e-03,  2.6388e-02,\n",
       "         -1.2052e-02, -1.6152e-02,  2.2690e-02,  1.7627e-03,  6.1199e-03,\n",
       "         -1.4805e-02,  1.5896e-02, -1.7562e-02, -1.8071e-02,  2.3627e-02,\n",
       "         -2.5006e-02,  3.4753e-02,  1.8548e-02,  2.3151e-02,  3.2189e-02,\n",
       "         -3.0121e-02,  9.2859e-03, -3.6863e-03,  2.5707e-02, -3.2331e-02,\n",
       "         -3.3812e-02,  1.1984e-02, -3.3655e-02,  6.1315e-03, -7.6221e-03,\n",
       "         -2.9846e-02, -9.1936e-03,  9.1177e-03, -1.9420e-02,  1.1690e-02,\n",
       "         -1.0241e-02, -6.3828e-03, -9.7345e-03, -3.0611e-02, -7.1241e-03,\n",
       "          5.1379e-03,  1.2288e-02,  2.0985e-03, -1.7825e-02,  3.2022e-02,\n",
       "         -6.1982e-03,  4.7776e-03,  3.2063e-02, -9.0743e-03, -5.0005e-03,\n",
       "          2.4767e-02, -3.3996e-02, -1.2416e-02, -1.0714e-02, -2.4087e-02,\n",
       "         -7.4106e-03, -2.0878e-02, -3.4359e-02, -4.7129e-03, -1.2826e-03,\n",
       "          2.1348e-03, -2.3112e-02,  1.8756e-02,  3.4428e-02,  8.7785e-05,\n",
       "          4.5163e-03,  1.5337e-02, -2.2977e-02,  3.2359e-02, -1.7315e-02,\n",
       "         -1.2814e-02,  3.4082e-02,  1.5139e-02, -1.1697e-02,  2.0879e-02,\n",
       "         -1.5451e-02,  4.1879e-03, -7.8814e-03,  3.5699e-02, -1.0011e-02,\n",
       "          2.0679e-02, -1.5130e-02,  3.0707e-02, -1.6681e-03, -2.4830e-02,\n",
       "          3.5693e-02,  3.3824e-02, -1.1479e-02, -1.6705e-02, -1.8688e-04,\n",
       "         -2.2340e-02,  2.9499e-02, -1.6970e-02, -1.4383e-02,  2.1938e-02,\n",
       "          5.5886e-03, -2.8515e-02, -2.9983e-02, -2.5460e-02,  1.4958e-02,\n",
       "         -2.1461e-02,  3.2465e-03,  9.6193e-03,  3.5533e-03,  9.3797e-03,\n",
       "          7.8214e-03,  1.1548e-02,  7.3334e-03, -2.6850e-02,  4.8814e-03,\n",
       "          2.2163e-02, -1.1754e-02,  1.5822e-02, -2.7876e-02, -6.0997e-03,\n",
       "          2.4056e-02, -9.0477e-03,  2.0126e-03,  1.7444e-03,  1.8022e-02,\n",
       "          1.2128e-02, -2.3359e-02, -8.9644e-03, -1.6968e-02,  1.9640e-02,\n",
       "         -2.7201e-02,  2.4851e-02,  1.9479e-02,  4.9585e-04, -1.3529e-02,\n",
       "         -3.4079e-03, -2.9030e-02,  3.3321e-02, -1.0940e-02,  1.5552e-02,\n",
       "         -3.4922e-02,  9.5609e-04], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0388, -0.0257, -0.0164,  ..., -0.0413, -0.0406,  0.0300],\n",
       "         [-0.0019,  0.0418,  0.0241,  ...,  0.0299, -0.0348,  0.0351],\n",
       "         [ 0.0285,  0.0222, -0.0181,  ...,  0.0093,  0.0192, -0.0270],\n",
       "         ...,\n",
       "         [ 0.0416,  0.0078,  0.0153,  ..., -0.0190, -0.0282,  0.0191],\n",
       "         [ 0.0349, -0.0021, -0.0098,  ..., -0.0116, -0.0009,  0.0249],\n",
       "         [ 0.0307,  0.0292,  0.0114,  ..., -0.0072, -0.0128,  0.0076]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 1.7880e-03, -4.2152e-02, -3.1449e-02,  3.8472e-02,  2.3783e-03,\n",
       "          2.9853e-02, -3.2470e-02,  1.9020e-02,  4.3015e-02, -2.8742e-02,\n",
       "         -1.5520e-02, -2.6696e-02,  2.3498e-02,  6.5613e-03, -2.5007e-02,\n",
       "         -1.6182e-02, -1.4500e-02, -7.2253e-03, -1.1596e-02,  3.7512e-02,\n",
       "          3.0666e-02,  3.0153e-02, -1.8417e-02, -8.0722e-03, -1.0982e-02,\n",
       "         -2.8094e-02,  3.1375e-02,  4.0156e-02,  2.8136e-02,  1.7734e-02,\n",
       "         -3.6865e-02,  3.2016e-02, -2.9018e-02, -3.9524e-02, -4.2182e-02,\n",
       "          2.5388e-02,  3.4016e-02, -2.2035e-03, -2.4865e-02, -2.7768e-04,\n",
       "          2.6379e-02,  2.0900e-02,  3.9878e-03, -2.2663e-03,  2.1697e-02,\n",
       "         -2.6535e-03, -1.5938e-02,  8.8704e-03,  1.7353e-02,  1.7799e-02,\n",
       "          5.4464e-03,  2.1188e-02, -3.3426e-02, -1.3137e-02, -2.1261e-02,\n",
       "          3.0911e-02, -4.2838e-04, -3.0166e-02, -3.9079e-02,  6.0135e-03,\n",
       "          2.1238e-02, -4.1560e-02, -3.4455e-03,  1.3512e-02, -3.2629e-02,\n",
       "          2.5872e-02,  1.4842e-02, -3.6953e-02,  2.8089e-02, -2.1840e-02,\n",
       "         -2.9747e-02,  3.6064e-02,  2.6796e-03,  2.6268e-02, -3.9635e-02,\n",
       "         -8.0800e-03, -1.2599e-02, -1.3174e-02, -3.4269e-02, -1.2280e-02,\n",
       "          1.1910e-02, -3.9443e-02,  1.0648e-02,  4.2598e-02,  3.0030e-02,\n",
       "          2.2825e-02, -4.1658e-02,  4.3455e-03,  2.1038e-02,  3.0982e-02,\n",
       "         -3.0662e-02, -3.7964e-02,  3.9327e-02,  1.2076e-02,  2.7837e-02,\n",
       "         -2.0360e-02, -2.4979e-02, -2.3195e-02,  1.5237e-03,  1.1303e-02,\n",
       "         -1.9014e-02, -3.6295e-02,  9.3600e-03, -2.2223e-02,  4.0857e-02,\n",
       "          4.3109e-03, -7.7919e-03,  3.0938e-02,  2.0081e-02,  2.6801e-03,\n",
       "          7.5437e-03, -1.8998e-02,  1.1172e-02, -3.0424e-03,  1.5688e-02,\n",
       "         -5.5740e-03, -3.7990e-02,  4.3504e-02,  1.5086e-02, -2.5105e-03,\n",
       "          3.4513e-02,  3.9715e-02,  4.3195e-02,  2.3173e-02, -3.7756e-02,\n",
       "          3.5506e-02, -4.4106e-02,  1.2965e-02, -2.0585e-02,  9.8913e-03,\n",
       "          3.5478e-03, -1.0347e-03, -2.7260e-02, -1.4588e-02,  3.1452e-02,\n",
       "         -1.4525e-02,  1.1616e-03, -4.2836e-02,  7.6347e-03,  5.2528e-04,\n",
       "         -2.0599e-02,  3.7649e-02, -2.8261e-02, -4.1143e-02,  2.4628e-03,\n",
       "         -2.5520e-02, -3.6789e-02, -4.3948e-02,  4.3536e-02, -4.1946e-03,\n",
       "          1.5715e-02,  2.9454e-02, -3.9064e-02,  1.7067e-02,  4.1008e-03,\n",
       "         -2.5998e-02, -4.0240e-02, -4.4166e-02,  2.5690e-02,  1.2841e-02,\n",
       "          4.7644e-03, -4.3628e-02,  2.9763e-03,  1.6493e-02,  2.7658e-02,\n",
       "         -3.6728e-03,  3.4685e-02, -3.4971e-02,  3.5890e-02, -3.4490e-02,\n",
       "         -1.5731e-02, -1.1631e-02,  2.0057e-02,  4.5670e-03, -2.9232e-03,\n",
       "          3.4216e-03,  1.4367e-02, -2.8156e-02, -2.4574e-02,  3.0072e-02,\n",
       "          3.7133e-02, -1.5593e-02, -1.3543e-02,  1.8975e-02, -3.3470e-02,\n",
       "          2.3026e-02, -3.5058e-02, -2.5836e-02,  3.8271e-02,  3.0984e-02,\n",
       "          4.0543e-02,  2.2415e-02, -3.0366e-02,  1.3180e-03,  2.9721e-03,\n",
       "          1.0846e-02, -1.0611e-02,  1.3024e-03,  7.0535e-03, -3.0169e-02,\n",
       "          1.4128e-02,  2.5767e-02,  4.1287e-02, -5.9217e-03, -2.3720e-02,\n",
       "          3.4498e-02,  4.1508e-02,  1.7090e-02,  3.2466e-02, -3.1722e-02,\n",
       "         -1.3016e-02, -2.2651e-02,  2.4912e-02,  3.6833e-02, -3.1575e-02,\n",
       "          2.4679e-02, -3.6202e-03, -4.1333e-02,  4.4187e-02, -3.9033e-02,\n",
       "         -1.3902e-02, -4.1545e-03, -3.7599e-02,  3.7211e-02,  4.0305e-02,\n",
       "         -6.4476e-03,  3.8793e-02,  1.9613e-02, -3.2080e-02,  1.7122e-02,\n",
       "         -4.0836e-03,  4.4162e-02, -5.5891e-04,  2.3471e-02, -4.1540e-04,\n",
       "         -1.3711e-02,  1.7115e-02,  3.2839e-02,  3.7665e-02, -9.4103e-03,\n",
       "          1.4168e-02,  2.7482e-02, -2.3713e-02, -3.8922e-02, -3.6970e-02,\n",
       "         -2.7233e-02, -8.1244e-03, -3.0908e-02,  7.7537e-03, -3.6551e-02,\n",
       "         -2.1993e-02,  3.5871e-02, -3.0189e-02, -3.2014e-02, -9.8756e-03,\n",
       "         -2.5146e-02, -1.7474e-02,  8.5337e-03,  2.3764e-02,  4.1348e-02,\n",
       "         -1.6325e-02,  4.2979e-02, -1.6833e-02,  3.8822e-02,  1.2947e-03,\n",
       "         -7.0223e-03, -6.5568e-03, -3.3375e-02,  2.0277e-02, -3.9359e-02,\n",
       "          1.4597e-02, -2.7786e-02, -7.3353e-03,  2.2846e-02, -8.6671e-03,\n",
       "          3.2601e-02,  2.5290e-02,  3.8745e-02, -1.9180e-02, -6.2184e-03,\n",
       "          4.4098e-02, -2.1706e-02, -2.3907e-02,  1.4787e-02, -2.4526e-02,\n",
       "          2.5558e-02, -3.2624e-02,  1.3875e-02, -1.4206e-02, -8.7958e-03,\n",
       "          3.3586e-02,  2.5117e-02,  7.6890e-03,  2.8837e-02, -2.8823e-02,\n",
       "          3.9076e-02,  3.2924e-02,  4.0561e-02,  1.8964e-02, -1.2750e-02,\n",
       "         -1.8078e-02, -4.0519e-02,  1.9462e-02,  3.8073e-02, -4.7624e-03,\n",
       "         -1.9143e-02, -5.4348e-03,  3.6145e-02,  3.2412e-02, -3.6432e-02,\n",
       "         -3.1223e-03, -4.1108e-02, -4.2134e-03,  8.2387e-03,  2.7749e-02,\n",
       "          3.6748e-02, -1.1490e-03, -3.4290e-02, -1.2696e-02,  3.9239e-02,\n",
       "          2.3470e-02, -4.9840e-03,  3.7678e-02, -3.8339e-03, -1.0790e-03,\n",
       "         -2.7696e-02, -8.7077e-03, -3.9944e-02, -5.8738e-03, -3.2252e-03,\n",
       "         -5.5572e-03,  2.6471e-02,  1.7793e-02, -3.5826e-02,  1.4682e-02,\n",
       "          3.8547e-02, -4.0667e-02,  4.3711e-02, -3.3439e-02, -2.0036e-02,\n",
       "         -4.0262e-02,  1.6953e-02, -2.2815e-02,  2.7196e-02, -8.6735e-03,\n",
       "          2.6834e-02,  2.4487e-02, -6.2311e-03, -2.5995e-02,  4.1024e-03,\n",
       "          3.0487e-02,  3.6750e-02, -3.1118e-02,  3.2451e-02,  1.2953e-02,\n",
       "         -3.7902e-02, -1.9226e-02, -3.7943e-02, -2.6389e-02,  2.8363e-02,\n",
       "          2.2023e-02,  2.3398e-02, -2.8717e-02,  2.6318e-02,  1.6199e-02,\n",
       "         -2.4938e-02, -1.0241e-02,  4.4145e-02, -1.2315e-02, -1.1101e-02,\n",
       "          3.0977e-02, -3.7644e-02,  1.8167e-02, -2.9888e-03,  3.7474e-02,\n",
       "          3.3756e-02,  4.3233e-02,  1.0803e-02, -4.1170e-02,  6.0244e-03,\n",
       "         -3.1446e-02,  4.1442e-03, -2.4743e-02,  9.7863e-03, -1.9936e-02,\n",
       "          4.5035e-04,  1.9550e-02,  5.7485e-03,  8.4979e-03, -5.3677e-03,\n",
       "          1.8799e-02, -2.0175e-02,  7.6362e-03, -2.2651e-02,  1.6212e-02,\n",
       "          1.2711e-02,  2.6644e-03, -2.8629e-02, -9.1818e-03,  2.5455e-02,\n",
       "         -6.8287e-03,  3.6066e-02,  6.0085e-03,  2.7826e-02, -3.0823e-02,\n",
       "         -2.1325e-02,  2.2636e-02, -3.7180e-02, -2.1226e-02, -2.5677e-02,\n",
       "          7.8408e-03,  2.0907e-02, -1.2928e-02, -3.4916e-02,  1.0506e-02,\n",
       "         -1.2199e-02,  4.0326e-02,  5.6527e-03,  1.2914e-02,  8.6806e-03,\n",
       "          4.0196e-02,  2.9974e-02,  1.1212e-02,  3.4862e-02, -2.7201e-02,\n",
       "          3.5358e-02,  3.3516e-02, -2.1953e-03, -3.5880e-03,  7.6442e-03,\n",
       "          2.6907e-02,  2.5169e-02, -4.1763e-03,  2.5247e-03,  1.9938e-02,\n",
       "          1.2612e-02,  9.2593e-03, -3.1063e-02, -3.4211e-02,  2.7270e-02,\n",
       "         -1.0976e-03,  3.9531e-03,  1.2784e-02, -8.0960e-03, -7.1261e-03,\n",
       "          3.7412e-02, -1.1221e-02,  1.9203e-02,  1.9990e-02,  1.6562e-02,\n",
       "          1.9782e-02, -1.0225e-02, -2.2334e-02,  1.2313e-02, -2.3140e-02,\n",
       "         -7.7874e-03,  1.3489e-02,  5.6774e-03,  2.6038e-02, -4.0411e-02,\n",
       "          1.1645e-02,  4.1962e-02,  3.6183e-02,  3.0422e-02,  7.9668e-03,\n",
       "         -2.0020e-02, -2.4924e-02, -1.8238e-02, -2.6679e-02,  4.3046e-03,\n",
       "         -3.7079e-02,  2.2155e-02,  2.6470e-02,  1.1632e-02, -3.2322e-02,\n",
       "         -1.3561e-02, -1.9303e-05,  2.3154e-02, -4.2255e-02, -4.2251e-02,\n",
       "          3.3315e-02, -4.3439e-02,  2.7329e-02, -1.0197e-02,  3.6132e-02,\n",
       "          3.0807e-02, -1.6943e-02,  2.4849e-02, -8.3073e-03,  3.8185e-02,\n",
       "          1.3951e-02, -5.7187e-03, -4.0560e-02, -1.2970e-02, -2.2295e-02,\n",
       "         -2.7692e-02, -2.3641e-02, -9.5820e-03,  1.5223e-02,  1.7795e-02,\n",
       "          1.1054e-03,  1.2102e-02,  2.6826e-02,  1.5046e-02, -3.8727e-02,\n",
       "         -3.8242e-02,  3.0452e-02, -1.7246e-03, -8.0642e-03,  1.9186e-02,\n",
       "         -2.4115e-02, -4.2525e-02], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-4.4004e-03, -3.4153e-02, -8.6743e-05,  ..., -3.3572e-02,\n",
       "           4.1160e-03,  2.2111e-05],\n",
       "         [-1.1445e-02, -1.3968e-02, -1.8135e-02,  ...,  4.1381e-02,\n",
       "           4.2200e-02, -2.3597e-03],\n",
       "         [-7.6412e-03, -2.5458e-02,  3.2088e-02,  ...,  2.9849e-02,\n",
       "          -1.2957e-02,  3.0763e-02],\n",
       "         ...,\n",
       "         [ 2.1653e-02,  3.7148e-02, -1.7028e-02,  ...,  2.1945e-02,\n",
       "          -1.1469e-03,  4.5286e-03],\n",
       "         [-2.8502e-02, -1.4263e-02,  3.6084e-02,  ..., -4.0492e-02,\n",
       "          -2.0232e-02,  1.4266e-02],\n",
       "         [ 5.6287e-03,  2.0429e-02, -8.2602e-03,  ..., -4.1828e-03,\n",
       "          -4.3618e-02,  2.0443e-02]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0230,  0.0020, -0.0202, -0.0354, -0.0229, -0.0022, -0.0281,  0.0107,\n",
       "          0.0279, -0.0023], requires_grad=True)]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8b2c08-ad94-4c83-88bc-0ca40dd04649",
   "metadata": {},
   "source": [
    "Pytorch provee también funciones de costo, como NLL (Negative Log Likehood). NLLLoss() asume que recibe log probs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "09db132e-ad2d-4eff-8781-43853c4dd0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3053, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func = nn.NLLLoss()\n",
    "loss = loss_func(log_probs_batch, y_batch)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540a2862-30f4-44e6-989d-75873ea9b589",
   "metadata": {},
   "source": [
    "Otra abstracción que provee Pytorch es la de optimizador. \n",
    "Un optimizador toma los pesos cuando es construido (también se dice que se registran los pesos en el optimizador) y oculta la forma en la que los pesos son actualizados teniendo en cuenta los gradientes: sólo basta tener los gradientes calculados dentro de los pesos (haber llamado a loss.backward()) y luego llamar al método .step(), que hará la actualización sobre los pesos registrados.\n",
    "\n",
    "Existen cálculos de actualización de los pesos (que también hacen uso del gradiente del costo respecto a los mismos), que mejoran la convergencia. Usando un optimizador en vez de hacer la cuenta manualmente hace que podamos cambiar\n",
    "\n",
    "Otra funcionalidad que provee el optimizador es setear en cero los gradientes de los parámetros registrados. Recordar que de no hacerlo se acumularán los gradientes entre llamadas sucesivas a backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "12d61a78-f855-4736-85d3-a022c0249ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([[-2.7826e-04, -2.6213e-04, -1.3763e-04,  ...,  3.4854e-05,\n",
      "          0.0000e+00, -3.5398e-04],\n",
      "        [-2.6385e-04, -1.0901e-04,  6.0954e-04,  ...,  1.7323e-04,\n",
      "          0.0000e+00, -3.8353e-04],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 1.4630e-05,  6.9074e-04,  7.8683e-04,  ...,  3.7135e-04,\n",
      "          0.0000e+00,  3.4842e-04],\n",
      "        [-1.5813e-04,  1.5044e-04,  4.3805e-04,  ...,  2.0042e-04,\n",
      "          0.0000e+00, -4.0906e-04],\n",
      "        [ 1.7474e-05, -3.6681e-04, -2.9460e-04,  ..., -5.6863e-05,\n",
      "          0.0000e+00, -4.2107e-06]])\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Seteamos los gradientes de los parámetros en cero\n",
    "optimizer.zero_grad()\n",
    "print(list(model.parameters())[0].grad)\n",
    "\n",
    "# Calculamos las predicciones, el costo y los gradientes\n",
    "probs_batch = model(x_batch)\n",
    "loss_func = nn.NLLLoss()\n",
    "loss = loss_func(log_probs_batch, y_batch)\n",
    "loss.backward()\n",
    "print(list(model.parameters())[2].grad)\n",
    "\n",
    "# Hacemos un paso de optimización\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "312ee8bb-1467-4696-944a-b61818feaa13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2177, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(predictions, y_valid_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e94c4d1-1271-4c6f-bbae-bcb37d327c05",
   "metadata": {},
   "source": [
    "Ya estamos en condiciones de escribir nuestro training loop usando Pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "3871e714-3508-4660-940d-949aff5ef44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss 2.218994081972506 | validation loss 2.2654674997409034 | accuracy 0.6078274760383386\n",
      "epoch 1 | train loss 2.0315826457148543 | validation loss 2.1427781314935275 | accuracy 0.7012779552715654\n",
      "epoch 2 | train loss 1.6119085942594389 | validation loss 1.851045632164065 | accuracy 0.7386182108626198\n",
      "epoch 3 | train loss 1.1142494590899434 | validation loss 1.3720462188763414 | accuracy 0.790535143769968\n",
      "epoch 4 | train loss 0.8042411773730391 | validation loss 0.9746000539852272 | accuracy 0.8353634185303515\n",
      "epoch 5 | train loss 0.6390524514186116 | validation loss 0.7510703354215896 | accuracy 0.8544329073482428\n",
      "epoch 6 | train loss 0.5422922998381118 | validation loss 0.6264618161543775 | accuracy 0.8699081469648562\n",
      "epoch 7 | train loss 0.48204053600375263 | validation loss 0.5502494796872215 | accuracy 0.8802915335463258\n",
      "epoch 8 | train loss 0.4417598305133204 | validation loss 0.49969196352948003 | accuracy 0.8848841853035144\n",
      "epoch 9 | train loss 0.41237472411923515 | validation loss 0.4637933694736666 | accuracy 0.8921725239616614\n",
      "epoch 10 | train loss 0.39042073597732824 | validation loss 0.4371734072840984 | accuracy 0.8971645367412141\n",
      "epoch 11 | train loss 0.37343692581969706 | validation loss 0.416488407288159 | accuracy 0.8986621405750799\n",
      "epoch 12 | train loss 0.3594333369034929 | validation loss 0.3998105963328597 | accuracy 0.9015575079872205\n",
      "epoch 13 | train loss 0.3489252862553246 | validation loss 0.3861052956734799 | accuracy 0.9034544728434505\n",
      "epoch 14 | train loss 0.3392548935291485 | validation loss 0.37479155525917895 | accuracy 0.9053514376996805\n",
      "epoch 15 | train loss 0.33010615551243194 | validation loss 0.3649329055179332 | accuracy 0.9074480830670927\n",
      "epoch 16 | train loss 0.32286720956190706 | validation loss 0.3561781415774207 | accuracy 0.9102436102236422\n",
      "epoch 17 | train loss 0.316290469001086 | validation loss 0.3483594974558932 | accuracy 0.9095447284345048\n",
      "epoch 18 | train loss 0.30984408953509773 | validation loss 0.34120841110297984 | accuracy 0.9123402555910544\n",
      "epoch 19 | train loss 0.30483005750483977 | validation loss 0.3347444591217901 | accuracy 0.9141373801916933\n",
      "epoch 20 | train loss 0.2998749040448056 | validation loss 0.32869153946962254 | accuracy 0.9159345047923323\n",
      "epoch 21 | train loss 0.2948949220129095 | validation loss 0.32318032461904367 | accuracy 0.9163338658146964\n",
      "epoch 22 | train loss 0.29104337289024845 | validation loss 0.3178226878736657 | accuracy 0.9170327476038339\n",
      "epoch 23 | train loss 0.28630049579059735 | validation loss 0.3127911125005283 | accuracy 0.919229233226837\n",
      "epoch 24 | train loss 0.28256701742307827 | validation loss 0.3081621666577712 | accuracy 0.9205271565495208\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "\n",
    "for idx_epoch in range(n_epochs):\n",
    "    # Loop de entrenamiento\n",
    "    loss_train_sum = 0\n",
    "    n_batches_train = 0\n",
    "    for x_train_batch, y_train_batch in train_dataloader:\n",
    "        predictions = model(x_train_batch)\n",
    "        loss = loss_func(predictions, y_train_batch)\n",
    "        loss_train_sum += loss.item()\n",
    "        n_batches_train += 1\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluamos los datos en validación\n",
    "    loss_validation_sum = 0\n",
    "    accuracy_sum = 0\n",
    "    for x_valid_batch, y_valid_batch in valid_dataloader:\n",
    "        predictions = model(x_valid_batch)\n",
    "        loss = loss_func(predictions, y_valid_batch)\n",
    "        loss_validation_sum += loss.item()\n",
    "        accuracy_sum += accuracy(predictions, y_valid_batch).item()\n",
    "    \n",
    "    # Imprimimos el loss en train y validación y la métrica (siempre en validación)\n",
    "    accuracy_validation = accuracy_sum / n_batches_valid\n",
    "    loss_validation = loss_validation_sum / n_batches_valid\n",
    "    train_validation = loss_train_sum / n_batches_train\n",
    "    print(f'epoch {idx_epoch} | train loss {loss_validation} | validation loss {train_validation} | accuracy {accuracy_validation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b9264c-9bf7-462a-9bf3-012ee9d951d9",
   "metadata": {},
   "source": [
    "La ventaja no es sólo que queda más corto, además está más \"encapsulado\": es más fácil de cambiar alguna de sus partes sin introducir cambios en otras partes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87811a6f-5d7f-4947-8c7e-e93144b3a39d",
   "metadata": {},
   "source": [
    "#### Ejercicio opcional 2\n",
    "\n",
    "Re-escribir el código de la unidad anterior utilizando el modelo resnet34 con pesos entrenados para ImageNet.\n",
    "\n",
    "ResNet es una familia de arquitecturas de redes convolucionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "c3092baf-919c-4c80-88f9-8af04003a128",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.models\n",
    "\n",
    "resnet_model = torchvision.models.resnet34()\n",
    "resnet_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d8943d-f2f9-4460-97bd-11262c6b527d",
   "metadata": {},
   "source": [
    "No es necesario para este ejercicio saber de redes convolucionales. Basta saber que resnet espera que le pasen un tensor de dimensiones $(3,H,W)$ siendo $3$ la cantidad de canales, $H$ la altura de una imágen y $W$ la anchura.\n",
    "\n",
    "Para aplicarlo, debemos des-aplanar las 784 features que componene cada sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "a6bb8678-4121-450c-9a5f-4dbab2ca87fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 28, 28])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch_new = x_batch.reshape(-1,28,28)\n",
    "x_batch_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24c988a-9bbb-49a4-b546-06b6bea34127",
   "metadata": {},
   "source": [
    "Podemos crear un ResNet con 1 solo canal, pero como vamos a usar un ResNet con pesos ya entrenado en el dataset ImageNet (que es un dataset a color), crearemos 3 canales iguales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "35a32941-acab-4636-abbe-4cb92ad3af64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 28, 28])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch_new = torch.stack([x_batch_new,x_batch_new,x_batch_new]).permute(1,0,2,3)\n",
    "x_batch_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fbe2eb-1253-4d94-a7e0-0249d555224a",
   "metadata": {},
   "source": [
    "Corroboremos que de des-aplanó correctamente mostrando la primera imágen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "f286621b-4726-4270-9a85-767a01f0e54f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.imshow(x_batch_new[0,0], cmap=\"gray\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8c3df9-c5aa-4c3b-84f7-03dd58fed04a",
   "metadata": {},
   "source": [
    "Finalmente podemos calcular la predicción del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "c2adf3fe-bec8-4f21-8ba3-15f1489d093c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 10]),\n",
       " tensor([ 1.8148,  1.0982, -0.2982,  1.5775, -1.2970,  0.8447,  0.2607,  1.7301,\n",
       "         -0.2534, -0.3613], grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs = resnet_model(x_batch_new)\n",
    "logprobs.shape, logprobs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac612bf-592f-4ad1-bbce-91112b321bbc",
   "metadata": {},
   "source": [
    "Como resnet_model está entrenado en ImageNet, predice 1000 logprobs. Podemos cambiar la última capa lineal de resnet_model para adaptarlo a nuestro problema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "209e175b-346f-4b7c-af10-5fdd71dc9b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model.fc = nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "de0d2540-8630-4352-88c0-8be5ee972a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 10]),\n",
       " tensor([ 1.8148,  1.0982, -0.2982,  1.5775, -1.2970,  0.8447,  0.2607,  1.7301,\n",
       "         -0.2534, -0.3613], grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs = resnet_model(x_batch_new)\n",
    "logprobs.shape, logprobs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea0dd0a-664d-4956-a372-98acd4f99d78",
   "metadata": {},
   "source": [
    "Esta última capa es completamente nueva. Los pesos son completamente aleatorios. Cuándo esto sucede lo recomendable es entrenar algunas épocas esta capa solamente y luego entrenar toda la red. Esto se puede lograr pasando al optimizer sólo los parámetros de ésta última capa. Y luego crear otro optimizer y pasarle todos los parámetros del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cb70a5-c29e-4aef-82a5-57a7526961ed",
   "metadata": {},
   "source": [
    "#### Ejercicio opcional 3\n",
    "\n",
    "Crear un dataset de imágenes de tu interés y entrenar un clasificar ResNet (ver ejercicio anterior) en dicho dataset.\n",
    "\n",
    "Para crear un dataset se puede utilizar, por ejemplo la API de Bing para descargar automáticamente imágenes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
