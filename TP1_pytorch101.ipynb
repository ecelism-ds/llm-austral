{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduccion a Pytorch\n",
    "\n",
    "Pytorch es una libreria que permite trabajar con vectores y matrices de muchas dimensiones.\n",
    "\n",
    "En ese sentido es muy parecido a Numpy. En numpy a los vectores son llamados arrays. En pytorch se los llama tensores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1.,2.],[3.,4.]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El shape del tensor es una lista del tamano de cada dimension. Si trabajamos con matrices (como comumnmente se llama a los vectores 2D), cada \"fila\" tiene la misma longitud: no hay una fila mas corta que otra.\n",
    "\n",
    "Lo mismo vale para mas dimensiones. No podemos hacer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 2 at dim 1 (got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 2 at dim 1 (got 1)"
     ]
    }
   ],
   "source": [
    "torch.tensor([[1.,2.],[3.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Que introduce de nuevo pytorch sobre numpy\n",
    "Esencialmente dos cosas:\n",
    "- Paralelizacion en GPU\n",
    "- Calculo automatico de derivadas (gradiente)\n",
    "\n",
    "Estas funciones son muy deseables cuando trabajamos con redes neuronales. Aunque tambien hay librerias de, por ejemplo, algebra lineal que han aprovechado esto para ser escritas sobre pytorch y funcionar eficientemente.\n",
    "\n",
    "Pytorch tambien provee muchas de las funcionalidades necesarias para definir una red y entrenarla.\n",
    "\n",
    "Con la siguiente funcion podemos ver si tenemos una GPU disponible en nuestro sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comparar la velocidad entre ejecutar algo en CPU o usando GPU (si tenemos).\n",
    "\n",
    "Pytorch requiere mover explicitamente los tensores a la GPU, se puede hacer facilmente mediante el metodo .cuda()\n",
    "\n",
    "(Si se operan vectores que estan en la GPU con vectores que estan en la GPU causara un error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.2 ms ± 16.4 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "A = torch.rand(1000,1000)\n",
    "B = torch.rand(1000,1000)\n",
    "A@B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "636 ns ± 3.7 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "if torch.cuda.is_available():\n",
    "    A = A.cuda()\n",
    "    B = B.cuda()\n",
    "    A@B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio Opcional 1\n",
    "\n",
    "Ejecutar el codigo anterior en una GPU. Para ello se pueden utilizar las GPU de papparspace u otro proveedor cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Autograd\n",
    "\n",
    "Pytorch provee la funcionalidad \"recordar\" como cada vector fue calculado con el fin de computar el gradiente.\n",
    "\n",
    "Podemos definir vectores con ciertos valores y crear otros como resultado de operar los primeros y pytorch recordara el grafo de las computaciones.\n",
    "\n",
    "Supongamos que tenemos los vectores:\n",
    "\n",
    "$ a = 1 $\n",
    "\n",
    "$ b = 2 $\n",
    "\n",
    "$ c = 0 $\n",
    "\n",
    "Y definimos $m$, $n$ (capas intermedias), $p$ (una prediccion) y $l$ (un costo) como:\n",
    "\n",
    "$ m = a + b $\n",
    "\n",
    "$ n = max(b,c) $\n",
    "\n",
    "$ p = m \\times n $\n",
    "\n",
    "$ l = p^2 $\n",
    "\n",
    "Pytorch calculara los valores intermedios dado el valor de las hojas:\n",
    "\n",
    "$ m = 3 $\n",
    "\n",
    "$ n = 2 $\n",
    "\n",
    "$ p = 6 $\n",
    "\n",
    "$ l = 36 $\n",
    "\n",
    "Y a la vez construira el siguiente grafo de computaciones:\n",
    "\n",
    "![title](graph.png)\n",
    "\n",
    "Si $a$,$b$,$c$ son nuestros parametros y queremos minimizar el costo $l$. Querremos calcular: $\\large\\frac{\\partial l}{\\partial a}$, $\\large\\frac{\\partial l}{\\partial b}$, $\\large\\frac{\\partial l}{\\partial c}$\n",
    "\n",
    "A primera vista no es fácil calcular dichas derivadas. En general, usando la definición de los valores, podemos calcular la derivada de un valor en función de los valores que dependen de forma directa de él. Haciendo referencia al grafo de las computaciones (la imagen de arriba), podemos calcular el valor de $\\large\\frac{\\partial y}{\\partial x}$ si existe una arista que $x \\rightarrow y$. Así, podemos calcular las siguientes derivadas, ya que cada variable depende de forma directa una de la otra:\n",
    "\n",
    "$\\large\\frac{\\partial l}{\\partial p} = 2 \\times p = 12$\n",
    "\n",
    "$\\large\\frac{\\partial p}{\\partial m} = n = 2$\n",
    "\n",
    "$\\large\\frac{\\partial p}{\\partial n} = m = 3$\n",
    "\n",
    "$\\large\\frac{\\partial m}{\\partial a} = 1$\n",
    "\n",
    "$\\large \\frac{\\partial m}{\\partial b} = 1$\n",
    "\n",
    "$\\large \\frac{\\partial n}{\\partial b} = 1$\n",
    "\n",
    "$\\large \\frac{\\partial n}{\\partial c} = 0$\n",
    "\n",
    "Pero como hacemos para calcular las derivadas $\\large\\frac{\\partial l}{\\partial a}$, $\\large\\frac{\\partial l}{\\partial b}$, $\\large\\frac{\\partial l}{\\partial c}$ que son las que necesitamos para actualizar el gradiente? \n",
    "\n",
    "Centrémosnos por un momento en el caso de $\\large\\frac{\\partial l}{\\partial a}$. Si bien no tenemos una fórmula que relacione de manera directa $a$ con $l$, el valor de $l$ depende del valor de $a$: esto es debido a que el valor de $l$ depende de $p$, que a su vez depende de $m$, que por último depende de $a$.\n",
    "\n",
    "Apliquemos la regla de la cadena entre $p$ (que es una función de $a$), $l$ (que es función de $p$) y $a$:\n",
    "\n",
    "$\\Large\\frac{\\partial l}{\\partial m} = \\frac{\\partial l}{\\partial p} \\times \\frac{\\partial p}{\\partial m}$\n",
    "\n",
    "La regla de la cadena es un t\n",
    "\n",
    "Usando las definicion de cada valor y la regla de la cadena tenemos que:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$\\large\\frac{\\partial l}{\\partial m} = \\frac{\\partial l}{\\partial p} \\times \\frac{\\partial p}{\\partial m} = 12 \\times 2 = 24$\n",
    "\n",
    "\n",
    "\n",
    "$\\large\\frac{\\partial l}{\\partial n} = \\frac{\\partial l}{\\partial p} \\times \\frac{\\partial p}{\\partial n} = 12 \\times 3 = 36$\n",
    "\n",
    "\n",
    "\n",
    "$\\large\\frac{\\partial l}{\\partial a} = \\frac{\\partial l}{\\partial m} \\times \\frac{\\partial m}{\\partial a} = 24 \\times 1 = 24$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Para el caso de $b$ usamos la red de la cadena multivariada (es muy parecida a la regla del producto, de hecho la regla del producto es un caso particular de la regla de la cadena multivariada):\n",
    "\n",
    "$\\large\\frac{\\partial l}{\\partial b} = \\frac{\\partial l}{\\partial m} \\times \\frac{\\partial m}{\\partial b} + \\frac{\\partial l}{\\partial n} \\times \\frac{\\partial n}{\\partial b} = 24+36 = 60$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.], requires_grad=True),\n",
       " tensor([2.], requires_grad=True),\n",
       " tensor([0.], requires_grad=True))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1.],requires_grad=True)\n",
    "b = torch.tensor([2.],requires_grad=True)\n",
    "c = torch.tensor([0.],requires_grad=True)\n",
    "a,b,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.], grad_fn=<AddBackward0>),\n",
       " tensor([2.], grad_fn=<MaximumBackward0>))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = a+b\n",
    "n = torch.max(a,b)\n",
    "m, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([6.], grad_fn=<MulBackward0>), tensor([36.], grad_fn=<PowBackward0>))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = m*n\n",
    "l = p**2\n",
    "p, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.retain_grad()\n",
    "n.retain_grad()\n",
    "p.retain_grad()\n",
    "l.retain_grad()\n",
    "\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.]), tensor([12.]))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.grad, p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([24.]), tensor([36.]))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.grad, n.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([24.]), tensor([60.]), None)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad, b.grad, c.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretación del gradiente\n",
    "\n",
    "Recordar que el gradiente $\\large \\frac{\\partial l}{\\partial a}$ indica cuánto cambia $l$ ante pequeños cambios de $a$:\n",
    "\n",
    "$\\large \\frac{\\partial l}{\\partial a} = lim_{\\Delta a \\rightarrow 0} \\frac{\\Delta l}{\\Delta a}$\n",
    "\n",
    "Corroboraremos esto empíricamente, calculando $\\Delta l$ para variaciones $\\Delta a$, $\\Delta b$, $\\Delta c$ muy pequeñas.\n",
    "\n",
    "Para ellos haremos una función que nos calcule $l$ dado $a$, $b$ y $c$c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_l(a,b,c):\n",
    "    m = a+b\n",
    "    n = torch.max(a,b)\n",
    "    p = m*n\n",
    "    l = p**2\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chequeamos que el resultado es efectivamente 36:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([36.], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ov = calculate_l(a, b, c)\n",
    "ov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego calcularemos $l$ introduciendo un pequeño cambio en $a$. Observaremos como resulta el ratio de cambio $\\frac{\\Delta l}{\\Delta a}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New value: 36.02400207519531\n",
      "Change value: 0.0240020751953125\n",
      "Change ration: 24.002073287963867\n"
     ]
    }
   ],
   "source": [
    "small_change = 0.001\n",
    "nv = calculate_l(a + small_change, b, c)\n",
    "print(f'New value: {(nv).item()}')\n",
    "print(f'Change value: {(nv - ov).item()}')\n",
    "print(f'Change ration: {((nv - ov) / small_change).item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efectivamente el ratio de variación es 24, como la derivada $\\large \\frac{\\partial l}{\\partial a}$.\n",
    "Haremos lo mismo para $b$ y para $c$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New value: 36.06003189086914\n",
      "Change value: 0.060031890869140625\n",
      "Change ration: 60.03188705444336\n"
     ]
    }
   ],
   "source": [
    "nv = calculate_l(a , b + small_change, c)\n",
    "nv, (nv - ov) / small_change\n",
    "print(f'New value: {(nv).item()}')\n",
    "print(f'Change value: {(nv - ov).item()}')\n",
    "print(f'Change ration: {((nv - ov) / small_change).item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New value: 36.0\n",
      "Change value: 0.0\n",
      "Change ration: 0.0\n"
     ]
    }
   ],
   "source": [
    "nv = calculate_l(a , b, c + small_change)\n",
    "nv, (nv - ov) / small_change\n",
    "print(f'New value: {(nv).item()}')\n",
    "print(f'Change value: {(nv - ov).item()}')\n",
    "print(f'Change ration: {((nv - ov) / small_change).item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio Opcional 2\n",
    "\n",
    "Dada las siguientes definiciones:\n",
    "\n",
    "$a = 2$\n",
    "\n",
    "$b = 3$\n",
    "\n",
    "$c = -1$\n",
    "\n",
    "$d = 7$\n",
    "\n",
    "$m = 3 \\times a + 4 \\times b - 2 \\times c + 10 \\times d$\n",
    "\n",
    "$n = 7 \\times a + b + \\times c + 4 \\times d$\n",
    "\n",
    "$p = max(m,0)$\n",
    "\n",
    "$q = max(n,0)$\n",
    "\n",
    "$z = p - q$\n",
    "\n",
    "$ pred = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "$loss = log(pred)$\n",
    "\n",
    "Calcular:\n",
    "\n",
    "1. El grafo de las computaciones (un diagrama de flechas como el de arriba donde se vea la relación de dependencia entre las variables).\n",
    "2. Las derivadas $\\large \\frac{\\partial loss}{\\partial a}$, $\\large \\frac{\\partial loss}{\\partial b}$, $\\large \\frac{\\partial loss}{\\partial c}$, $\\large \\frac{\\partial loss}{\\partial d}$ manualmente.\n",
    "3. Las derivadas $\\large \\frac{\\partial loss}{\\partial a}$, $\\large \\frac{\\partial loss}{\\partial b}$, $\\large \\frac{\\partial loss}{\\partial c}$, $\\large \\frac{\\partial loss}{\\partial d}$ utilizando Pytorch Autograd (backwards).\n",
    "4. Una aproximación de las derivadas $\\large \\frac{\\partial loss}{\\partial a}$, $\\large \\frac{\\partial loss}{\\partial b}$, $\\large \\frac{\\partial loss}{\\partial c}$, $\\large \\frac{\\partial loss}{\\partial d}$ mediante los ratios $\\large \\frac{\\Delta loss}{\\Delta a}$, $\\large \\frac{\\Delta loss}{\\Delta b}$, $\\large \\frac{\\Delta loss}{\\Delta c}$, $\\large \\frac{\\Delta loss}{\\Delta d}$ de cambio utilizando pequeñas variaciones $\\Delta a$, $\\Delta b$, $\\Delta c$, $\\Delta d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando descenso por gradiente para problemas de optimización\n",
    "\n",
    "El método de descenso por gradiente permite optimizar (maximizar o minimizar) cualquier función $f(w)$. Consiste comenzar en cualquier $w_0$ aleatorio y hacer iterativamente (una y otra vez) la siguiente actualización:\n",
    "\n",
    "$w_{t+1} = w_{t} - \\eta \\times f'(w_t)$\n",
    "\n",
    "Por ejemplo si queremos hallar el mínimo de la siguiente función cuadrática:\n",
    "\n",
    "$f(x) = (x-3)^2 + 10$\n",
    "\n",
    "Podemos hacerlo analíticamente: observar que es una parábola con vértice en $x=3$ y que este es el $x$ que arrojará valores mínimos. Particularmente:\n",
    "\n",
    "$f(3) = 10$\n",
    "\n",
    "\n",
    "Pero también podemos calcular el mínimo con el método de descenso por gradiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (x-3)**2+10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2463], requires_grad=True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.rand(1, requires_grad=True)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda ejecuta un paso de descenso por gradiente. Si la ejecutamos sucesivas veces se puede observar que el valor de $f$ se reduce:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(0.24625301361083984) = 17.58312225341797\n"
     ]
    }
   ],
   "source": [
    "def iteration_gradient_descent(learning_rate = 0.01, show_value=False):\n",
    "    value = f(w)\n",
    "    if show_value:\n",
    "        print(f'f({w.item()}) = {value.item()}')\n",
    "    value.backward()\n",
    "    w.data = w.data - learning_rate * w.grad\n",
    "    w.grad.zero_()\n",
    "\n",
    "iteration_gradient_descent(show_value=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutando dentro de un loop para hacer muchas iteraciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(0.3013279438018799) = 17.2828311920166\n",
      "f(2.6421031951904297) = 10.128089904785156\n",
      "f(2.952536106109619) = 10.002252578735352\n",
      "f(2.9937055110931396) = 10.000040054321289\n",
      "f(2.999164581298828) = 10.000000953674316\n",
      "f(2.9998891353607178) = 10.0\n",
      "f(2.9999847412109375) = 10.0\n",
      "f(2.9999942779541016) = 10.0\n",
      "f(2.9999942779541016) = 10.0\n",
      "f(2.9999942779541016) = 10.0\n"
     ]
    }
   ],
   "source": [
    "for iteration_number in range(1000):\n",
    "    iteration_gradient_descent(show_value = (iteration_number%100 == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente hemos encontrado el valor de $x$ que minimiza $f(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([10.]), tensor([3.0000]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(w.data), w.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otra aplicación de descenso por gradiente\n",
    "\n",
    "Vimos que descenso por gradiente sirve para encontrar el mínimo de funciones. Pero también es útil en problemas que queremos encontrar cierta incógnita. Veremos cómo se puede hallar la inversa de una matriz con descenso por gradiente.\n",
    "\n",
    "Recordar que si tenemos una matriz $A$:\n",
    "\\begin{bmatrix}\n",
    "    1 & 2 & 3 \\\\\n",
    "    0 & 1 & 4 \\\\\n",
    "    5 & 6 & 0\n",
    "\\end{bmatrix}\n",
    "Su inversa $A^{-1}$ es:\n",
    "\\begin{bmatrix}\n",
    "    -24 & 18 & 5 \\\\\n",
    "    20 & -15 & -4 \\\\\n",
    "    -5 & 4 & 1\n",
    "\\end{bmatrix}\n",
    "Debido a que el [producto](https://en.wikipedia.org/wiki/Matrix_multiplication) de ambas es la identidad $A \\times A^{-1} = I$. \n",
    "\n",
    "Recordar que la identidad $I$ es:\n",
    "\\begin{bmatrix}\n",
    "    1 & 0 & 0 \\\\\n",
    "    0 & 1 & 0 \\\\\n",
    "    0 & 0 & 1\n",
    "\\end{bmatrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [0., 1., 4.],\n",
       "        [5., 6., 0.]])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = [[1,2,3],\n",
    "          [0,1,4],\n",
    "          [5,6,0]]\n",
    "A = torch.tensor(values, dtype=torch.float)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen métodos para calcular la inversa de una matrix. Pytorch ya posee dicha funcionalidad como una función de librería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-24.0000,  18.0000,   5.0000],\n",
       "        [ 20.0000, -15.0000,  -4.0000],\n",
       "        [ -5.0000,   4.0000,   1.0000]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_inversed = torch.inverse(A)\n",
    "A_inversed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch posee además métodos para definir matrices útiles como la identidad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(A.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos corroborar que el producto de la inversa calculada por la matriz original es la identidad. Si se mira con atención en la diagonal hay todos $1$ y en las entradas que no pertenecen a la diagonal hay números muy pequeños (casi 0). Esto es producto a [errores numéricos](https://en.wikipedia.org/wiki/Numerical_error), (recordar que la representación como float no es exacta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00,  3.8147e-06,  0.0000e+00],\n",
       "        [ 0.0000e+00,  1.0000e+00, -3.8147e-06],\n",
       "        [ 4.7684e-07,  1.9073e-06,  1.0000e+00]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_inversed @ A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero cómo usamos descenso por gradiente para encontrar $A^{-1}$? No sirve sólo para problemas de minimización?\n",
    "\n",
    "$A_{-1}$ es la única matriz que multiplicada por $A$ da la identidad:\n",
    "\n",
    "$A \\times A_{-1} = I$\n",
    "\n",
    "Lo que es equivalente a decir:\n",
    "\n",
    "$A \\times A_{-1} - I = 0$\n",
    "\n",
    "Y a su vez, es equivalente a:\n",
    "\n",
    "$||A \\times A_{-1} - I ||_2 = 0$\n",
    "\n",
    "La norma $|| . ||_2$ es siempre positiva (o cero). Por lo cual, encontrar $A_{-1}$ es equivalente a minimizar:\n",
    "\n",
    "$f(W) = ||A \\times W - I ||_2 $\n",
    "\n",
    "Comenzaremos con una matriz $W_0$ aleatoria e iremos actualizando sus entradas con descenso por gradiente para minimizar $f$ definida tal como se muestra arriba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(W):\n",
    "    dim = A.shape[0]\n",
    "    I = torch.eye(dim)\n",
    "    return torch.norm(A @ W - I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro $W$ inicial está lejos de ser la inversa $A_{-1}$ buscada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7539, 0.6806, 0.3723],\n",
       "        [0.7387, 0.1340, 0.2240],\n",
       "        [0.3931, 0.3627, 0.0761]], requires_grad=True)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.rand((3,3), requires_grad=True)\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(W)$ que puede ser interpretado como \"el error\" de nuestra solución está lejos de ser $0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.3368, grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función que realiza una iteración de descenso por gradiente es igual que la que usamos para nuestra función cuadrática.\n",
    "También podemos corrobarar, una vez más que ejecutando sucesivas iteraciones el valor de $f$ es cada vez menor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(W) = 0.9843394160270691\n",
      "tensor([[-0.1036,  0.5337,  0.5128],\n",
      "        [ 0.1075, -0.4602, -0.2646],\n",
      "        [ 0.0896,  0.2799,  0.0443]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "def iteration_gradient_descent(learning_rate = 0.01, show_value=False):\n",
    "    value = f(W)\n",
    "    if show_value:\n",
    "        print(f'f(W) = {value.item()}')\n",
    "        print(W)\n",
    "    value.backward()\n",
    "    W.data = W.data - learning_rate * W.grad\n",
    "    W.grad.zero_()\n",
    "\n",
    "iteration_gradient_descent(show_value=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer varias iteraciones utilizaremos learning rates cada vez más chicos (recordar que al aproximarse al mínimo es conveniente reducir el learning rate para no \"saltar\" por encima del mínimo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(W) = 10.336766242980957\n",
      "tensor([[0.7539, 0.6806, 0.3723],\n",
      "        [0.7387, 0.1340, 0.2240],\n",
      "        [0.3931, 0.3627, 0.0761]], requires_grad=True)\n",
      "f(W) = 3.426985025405884\n",
      "tensor([[-20.2819,  15.0119,   4.1587],\n",
      "        [ 16.4554, -12.7515,  -3.4269],\n",
      "        [ -4.2405,   3.3464,   0.8116]], requires_grad=True)\n",
      "f(W) = 3.4278721809387207\n",
      "tensor([[-23.5711,  17.4188,   4.8127],\n",
      "        [ 19.1932, -14.7557,  -3.9718],\n",
      "        [ -4.9411,   3.8590,   0.9509]], requires_grad=True)\n",
      "f(W) = 3.4236056804656982\n",
      "tensor([[-24.1061,  17.8119,   4.9193],\n",
      "        [ 19.6390, -15.0825,  -4.0604],\n",
      "        [ -5.0550,   3.9427,   0.9736]], requires_grad=True)\n",
      "f(W) = 0.3344844877719879\n",
      "tensor([[-23.9776,  17.9600,   4.9859],\n",
      "        [ 19.9374, -14.9900,  -4.0007],\n",
      "        [ -4.9984,   3.9898,   0.9961]], requires_grad=True)\n",
      "f(W) = 0.3344844877719879\n",
      "tensor([[-23.9776,  17.9600,   4.9859],\n",
      "        [ 19.9374, -14.9900,  -4.0007],\n",
      "        [ -4.9984,   3.9898,   0.9961]], requires_grad=True)\n",
      "f(W) = 0.3344844877719879\n",
      "tensor([[-23.9776,  17.9600,   4.9859],\n",
      "        [ 19.9374, -14.9900,  -4.0007],\n",
      "        [ -4.9984,   3.9898,   0.9961]], requires_grad=True)\n",
      "f(W) = 0.03438796103000641\n",
      "tensor([[-23.9620,  17.9718,   4.9917],\n",
      "        [ 19.9638, -14.9789,  -3.9943],\n",
      "        [ -4.9922,   3.9938,   0.9981]], requires_grad=True)\n",
      "f(W) = 0.03438796103000641\n",
      "tensor([[-23.9620,  17.9718,   4.9917],\n",
      "        [ 19.9638, -14.9789,  -3.9943],\n",
      "        [ -4.9922,   3.9938,   0.9981]], requires_grad=True)\n",
      "f(W) = 0.03438796103000641\n",
      "tensor([[-23.9620,  17.9718,   4.9917],\n",
      "        [ 19.9638, -14.9789,  -3.9943],\n",
      "        [ -4.9922,   3.9938,   0.9981]], requires_grad=True)\n",
      "f(W) = 0.0034222565591335297\n",
      "tensor([[-23.9633,  17.9753,   4.9930],\n",
      "        [ 19.9699, -14.9792,  -3.9940],\n",
      "        [ -4.9922,   3.9947,   0.9985]], requires_grad=True)\n",
      "f(W) = 0.0034222565591335297\n",
      "tensor([[-23.9633,  17.9753,   4.9930],\n",
      "        [ 19.9699, -14.9792,  -3.9940],\n",
      "        [ -4.9922,   3.9947,   0.9985]], requires_grad=True)\n",
      "f(W) = 0.0034222565591335297\n",
      "tensor([[-23.9633,  17.9753,   4.9930],\n",
      "        [ 19.9699, -14.9792,  -3.9940],\n",
      "        [ -4.9922,   3.9947,   0.9985]], requires_grad=True)\n",
      "f(W) = 0.0014708051458001137\n",
      "tensor([[-23.9635,  17.9751,   4.9929],\n",
      "        [ 19.9696, -14.9793,  -3.9941],\n",
      "        [ -4.9922,   3.9947,   0.9985]], requires_grad=True)\n",
      "f(W) = 0.0014708051458001137\n",
      "tensor([[-23.9635,  17.9751,   4.9929],\n",
      "        [ 19.9696, -14.9793,  -3.9941],\n",
      "        [ -4.9922,   3.9947,   0.9985]], requires_grad=True)\n",
      "f(W) = 0.0014708051458001137\n",
      "tensor([[-23.9635,  17.9751,   4.9929],\n",
      "        [ 19.9696, -14.9793,  -3.9941],\n",
      "        [ -4.9922,   3.9947,   0.9985]], requires_grad=True)\n",
      "f(W) = 0.0014708051458001137\n",
      "tensor([[-23.9635,  17.9751,   4.9929],\n",
      "        [ 19.9696, -14.9793,  -3.9941],\n",
      "        [ -4.9922,   3.9947,   0.9985]], requires_grad=True)\n",
      "f(W) = 0.0014708051458001137\n",
      "tensor([[-23.9635,  17.9751,   4.9929],\n",
      "        [ 19.9696, -14.9793,  -3.9941],\n",
      "        [ -4.9922,   3.9947,   0.9985]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for lr_exponent in range(1,7):\n",
    "    n_iter_to_show = 100000\n",
    "    n_iter = n_iter_to_show * 3\n",
    "    learning_rate = 0.1 ** lr_exponent\n",
    "    for iteration_number in range(n_iter):\n",
    "        iteration_gradient_descent(show_value = (iteration_number%n_iter_to_show == 0), learning_rate = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El $W$ obtenido es una aproximacion relativamente buena de $A^{-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-23.9635,  17.9751,   4.9929],\n",
       "        [ 19.9696, -14.9793,  -3.9941],\n",
       "        [ -4.9922,   3.9947,   0.9985]], requires_grad=True)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-24.0000,  18.0000,   5.0000],\n",
       "        [ 20.0000, -15.0000,  -4.0000],\n",
       "        [ -5.0000,   4.0000,   1.0000]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_inversed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio Opcional 3\n",
    "\n",
    "Utilizar el método de descenso por gradiente para resolver el siguiente sistema de ecuaciones:\n",
    "\n",
    "$ 3 x + 4 y - 2 z = 0$\n",
    "\n",
    "$ 2 x - 3 y + 4 z = 11$\n",
    "\n",
    "$ x - 2 y + 3 z = 7$\n",
    "\n",
    "Hint: recordar la representación matricial del sistema de ecuaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "batch_size = 32\n",
    "train_kwargs = {'batch_size': 32}\n",
    "test_kwargs = {'batch_size': 32}\n",
    "\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,\n",
    "                   transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3145931182.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[108], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i_epoc in range(n_epoch):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 28, 28])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x,y in train_loader:\n",
    "    break\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 784])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(32,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shapea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1[3000][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1[3000][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_image = dataset1[3000][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa1ea117090>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(  tensor_image.permute(1, 2, 0)  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mueble: 0\n",
    "# Repuesto: 1\n",
    "# Herramienta: 2\n",
    "\n",
    "# Mueble: [1,0,0]\n",
    "# Repuesto: [0,1,0]\n",
    "# Herramienta: [0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texto : R^size(diccionario)\n",
    "# Texto[i] = cant de veces que aparece la palabra i-esima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word : R^size(diccionario)\n",
    "# Word[i] = 1 si y solo si la palabra en cuestion esta en la posicion i-esima\n",
    "# \"perro\" si esta en la posicion 20 [0,0,0 ... , 1 , 0, 0, ... 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = si queremos meter texto en una _ neuronal necesitamos representarlo como un vector\n",
    "\n",
    "texto[i]: la palabra en la posicion i-esima\n",
    "    \n",
    "vector[i] = el vector de la palabra texto[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
