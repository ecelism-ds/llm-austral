{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduccion a Pytorch\n",
    "\n",
    "Pytorch es una libreria que permite trabajar con vectores y matrices de muchas dimensiones.\n",
    "\n",
    "En ese sentido es muy parecido a Numpy. En numpy a los vectores son llamados arrays. En pytorch se los llama tensores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 2., 4.],\n",
       "         [3., 4., 7.]],\n",
       "\n",
       "        [[1., 2., 1.],\n",
       "         [3., 4., 4.]]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor([[[1.,2.,4.],[3.,4.,7.]],[[1.,2.,1.],[3.,4.,4.]]])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1.,2.],[3.,4.]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El shape del tensor es una lista del tamano de cada dimension. Si trabajamos con matrices (como comumnmente se llama a los vectores 2D), cada \"fila\" tiene la misma longitud: no hay una fila mas corta que otra.\n",
    "\n",
    "Lo mismo vale para mas dimensiones. No podemos hacer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 2 at dim 1 (got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1.\u001b[39m,\u001b[38;5;241m2.\u001b[39m],[\u001b[38;5;241m3.\u001b[39m]])\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 2 at dim 1 (got 1)"
     ]
    }
   ],
   "source": [
    "torch.tensor([[1.,2.],[3.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Que introduce de nuevo pytorch sobre numpy\n",
    "Esencialmente dos cosas:\n",
    "- Paralelizacion en GPU\n",
    "- Calculo automatico de derivadas (gradiente)\n",
    "\n",
    "Estas funciones son muy deseables cuando trabajamos con redes neuronales. Aunque tambien hay librerias de, por ejemplo, algebra lineal que han aprovechado esto para ser escritas sobre pytorch y funcionar eficientemente.\n",
    "\n",
    "Pytorch tambien provee muchas de las funcionalidades necesarias para definir una red y entrenarla.\n",
    "\n",
    "Con la siguiente funcion podemos ver si tenemos una GPU disponible en nuestro sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comparar la velocidad entre ejecutar algo en CPU o usando GPU (si tenemos).\n",
    "\n",
    "Pytorch requiere mover explicitamente los tensores a la GPU, se puede hacer facilmente mediante el metodo .cuda()\n",
    "\n",
    "(Si se operan vectores que estan en la GPU con vectores que estan en la GPU causara un error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.2 ms ± 16.4 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "A = torch.rand(1000,1000)\n",
    "B = torch.rand(1000,1000)\n",
    "A@B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "636 ns ± 3.7 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "if torch.cuda.is_available():\n",
    "    A = A.cuda()\n",
    "    B = B.cuda()\n",
    "    A@B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio Opcional 1\n",
    "\n",
    "Ejecutar el codigo anterior en una GPU. Para ello se pueden utilizar las GPU de papparspace u otro proveedor cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Autograd\n",
    "\n",
    "Pytorch provee la funcionalidad \"recordar\" como cada vector fue calculado con el fin de computar el gradiente.\n",
    "\n",
    "Podemos definir vectores con ciertos valores y crear otros como resultado de operar los primeros y pytorch recordara el grafo de las computaciones.\n",
    "\n",
    "Supongamos que tenemos los vectores:\n",
    "\n",
    "$ a = 1 $\n",
    "\n",
    "$ b = 2 $\n",
    "\n",
    "$ c = 0 $\n",
    "\n",
    "Y definimos $m$, $n$ (capas intermedias), $p$ (una prediccion) y $l$ (un costo) como:\n",
    "\n",
    "$ m = a + b $\n",
    "\n",
    "$ n = max(b,c) $\n",
    "\n",
    "$ p = m \\times n $\n",
    "\n",
    "$ l = p^2 $\n",
    "\n",
    "Pytorch calculara los valores intermedios dado el valor de las hojas:\n",
    "\n",
    "$ m = 3 $\n",
    "\n",
    "$ n = 2 $\n",
    "\n",
    "$ p = 6 $\n",
    "\n",
    "$ l = 36 $\n",
    "\n",
    "Y a la vez construira el siguiente grafo de computaciones:\n",
    "\n",
    "![title](graph.png)\n",
    "\n",
    "Si $a$,$b$,$c$ son nuestros parametros y queremos minimizar el costo $l$. Querremos calcular: $\\large\\frac{\\partial l}{\\partial a}$, $\\large\\frac{\\partial l}{\\partial b}$, $\\large\\frac{\\partial l}{\\partial c}$\n",
    "\n",
    "Usando las definicion de cada valor y la regla de la cadena tenemos que:\n",
    "\n",
    "$\\large\\frac{\\partial l}{\\partial p} = 2 \\times p = 12$\n",
    "\n",
    "$\\large\\frac{\\partial p}{\\partial m} = n = 2$\n",
    "\n",
    "$\\large\\frac{\\partial l}{\\partial m} = \\frac{\\partial l}{\\partial p} \\times \\frac{\\partial p}{\\partial m} = 12 \\times 2 = 24$\n",
    "\n",
    "$\\large\\frac{\\partial p}{\\partial n} = m = 3$\n",
    "\n",
    "$\\large\\frac{\\partial l}{\\partial n} = \\frac{\\partial l}{\\partial p} \\times \\frac{\\partial p}{\\partial n} = 12 \\times 3 = 36$\n",
    "\n",
    "$\\large\\frac{\\partial m}{\\partial a} = 1$\n",
    "\n",
    "$\\large\\frac{\\partial l}{\\partial a} = \\frac{\\partial l}{\\partial m} \\times \\frac{\\partial m}{\\partial a} = 24 \\times 1 = 24$\n",
    "\n",
    "$\\large \\frac{\\partial m}{\\partial b} = 1$\n",
    "\n",
    "$\\large \\frac{\\partial n}{\\partial b} = 1$\n",
    "\n",
    "Para el caso de $b$ usamos la red de la cadena multivariada (es muy parecida a la regla del producto, de hecho la regla del producto es un caso particular de la regla de la cadena multivariada):\n",
    "\n",
    "$\\large\\frac{\\partial l}{\\partial b} = \\frac{\\partial l}{\\partial m} \\times \\frac{\\partial m}{\\partial b} + \\frac{\\partial l}{\\partial n} \\times \\frac{\\partial n}{\\partial b} = 24+36 = 60$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.], requires_grad=True),\n",
       " tensor([2.], requires_grad=True),\n",
       " tensor([0.], requires_grad=True))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1.],requires_grad=True)\n",
    "b = torch.tensor([2.],requires_grad=True)\n",
    "c = torch.tensor([0.],requires_grad=True)\n",
    "a,b,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.], grad_fn=<AddBackward0>),\n",
       " tensor([2.], grad_fn=<MaximumBackward0>))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = a+b\n",
    "n = torch.max(a,b)\n",
    "m, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([6.], grad_fn=<MulBackward0>), tensor([36.], grad_fn=<PowBackward0>))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = m*n\n",
    "l = p**2\n",
    "p, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.retain_grad()\n",
    "n.retain_grad()\n",
    "p.retain_grad()\n",
    "l.retain_grad()\n",
    "\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.]), tensor([12.]))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.grad, p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([24.]), tensor([36.]))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.grad, n.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([24.]), tensor([60.]), None)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad, b.grad, c.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [0., 1., 4.],\n",
       "        [5., 6., 0.]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = [[1,2,3],\n",
    "          [0,1,4],\n",
    "          [5,6,0]]\n",
    "A = torch.tensor(values, dtype=torch.float)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(A.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-24.0000,  18.0000,   5.0000],\n",
       "        [ 20.0000, -15.0000,  -4.0000],\n",
       "        [ -5.0000,   4.0000,   1.0000]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_inversed = torch.inverse(A)\n",
    "A_inversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00, -1.9073e-06, -7.6294e-06],\n",
       "        [ 4.7684e-07,  1.0000e+00,  3.8147e-06],\n",
       "        [-5.9605e-07, -7.1526e-07,  1.0000e+00]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_inversed @ A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.2562e-06)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def distance(X,Y):\n",
    "    return torch.norm(X-Y)\n",
    "\n",
    "def distance_to_eye(X):\n",
    "    return distance(X,torch.eye(X.shape[0]))\n",
    "\n",
    "distance_to_eye(A_inversed @ A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5276, 0.0136, 0.5388],\n",
       "        [0.6386, 0.8579, 0.2926],\n",
       "        [0.9461, 0.7783, 0.3007]], requires_grad=True)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.rand((3,3), requires_grad=True)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.6429, 4.0643, 2.0262],\n",
       "        [4.4228, 3.9712, 1.4955],\n",
       "        [6.4692, 5.2154, 4.4496]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.0612, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "d = distance_to_eye(A @ W)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.9839, 2.4990, 1.5980],\n",
       "        [4.1890, 3.5148, 2.1760],\n",
       "        [2.3729, 1.9963, 0.9999]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.backward()\n",
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " W.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5276, 0.0136, 0.5388],\n",
       "        [0.6386, 0.8579, 0.2926],\n",
       "        [0.9461, 0.7783, 0.3007]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.039438143372535706\n",
      "0.039437923580408096\n",
      "0.039437923580408096\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(30000):\n",
    "    d = distance_to_eye(A @ W)\n",
    "    W.grad.zero_()\n",
    "    d.backward()\n",
    "    W.data = W.data - 0.00005 * W.grad\n",
    "    if iteration % 10000 == 0:\n",
    "        print(d.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-23.0651,  17.2730,   4.8100],\n",
       "        [ 19.2217, -14.3948,  -3.8419],\n",
       "        [ -4.8009,   3.8452,   0.9595]], requires_grad=True)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-24.0000,  18.0000,   5.0000],\n",
       "        [ 20.0000, -15.0000,  -4.0000],\n",
       "        [ -5.0000,   4.0000,   1.0000]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_inversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.8508e-01,  3.0317e-03, -1.0322e-01],\n",
       "        [ 1.2425e-02,  9.9748e-01,  8.5926e-02],\n",
       "        [-3.1794e-03,  6.4445e-04,  9.7801e-01]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W @ A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "train_kwargs = {'batch_size': 32}\n",
    "test_kwargs = {'batch_size': 32}\n",
    "\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,\n",
    "                   transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 28, 28])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x,y in train_loader:\n",
    "    break\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 784])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(32,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1[3000][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1[3000][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_image = dataset1[3000][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa1ea117090>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(  tensor_image.permute(1, 2, 0)  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mueble: 0\n",
    "# Repuesto: 1\n",
    "# Herramienta: 2\n",
    "\n",
    "# Mueble: [1,0,0]\n",
    "# Repuesto: [0,1,0]\n",
    "# Herramienta: [0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texto : R^size(diccionario)\n",
    "# Texto[i] = cant de veces que aparece la palabra i-esima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word : R^size(diccionario)\n",
    "# Word[i] = 1 si y solo si la palabra en cuestion esta en la posicion i-esima\n",
    "# \"perro\" si esta en la posicion 20 [0,0,0 ... , 1 , 0, 0, ... 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = si queremos meter texto en una _ neuronal necesitamos representarlo como un vector\n",
    "\n",
    "texto[i]: la palabra en la posicion i-esima\n",
    "    \n",
    "vector[i] = el vector de la palabra texto[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python imbue",
   "language": "python",
   "name": "imbue"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
